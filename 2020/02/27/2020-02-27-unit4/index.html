<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Tensorflow基础 | Iyin's blog</title><meta name="description" content="《Tensorflow深度学习》unit4"><meta name="keywords" content="Deep Learning,Tensorflow"><meta name="author" content="Iyin,yinwein@foxmail.com"><meta name="copyright" content="Iyin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/plant.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Tensorflow基础"><meta name="twitter:description" content="《Tensorflow深度学习》unit4"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="Tensorflow基础"><meta property="og:url" content="http://yoursite.com/2020/02/27/2020-02-27-unit4/"><meta property="og:site_name" content="Iyin's blog"><meta property="og:description" content="《Tensorflow深度学习》unit4"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="http://yoursite.com/2020/02/27/2020-02-27-unit4/"><link rel="prev" title="Tensorflow进阶" href="http://yoursite.com/2020/02/27/2020-02-27-unit5/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web:600&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Iyin's blog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://ae01.alicdn.com/kf/Hb280d3c31450463cb3dc638bf33ca871f.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据类型"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">数据类型</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（1）数值类型"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">（1）数值类型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（2）字符串类型"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">（2）字符串类型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（3）布尔类型"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">（3）布尔类型</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数值精度"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">数值精度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#待优化张量"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">待优化张量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#创建张量"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">创建张量</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（1）-从数组、列表对象创建"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">（1） 从数组、列表对象创建</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（2）创建全0或全1张量"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">（2）创建全0或全1张量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（3）创建自定义数值张量"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text">（3）创建自定义数值张量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（4）创建已知分布的张量"><span class="toc_mobile_items-number">4.4.</span> <span class="toc_mobile_items-text">（4）创建已知分布的张量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（5）创建序列"><span class="toc_mobile_items-number">4.5.</span> <span class="toc_mobile_items-text">（5）创建序列</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#张量的典型应用"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">张量的典型应用</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（1）标量"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">（1）标量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（2）向量"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">（2）向量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（3）矩阵"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">（3）矩阵</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（4）三维张量"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text">（4）三维张量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（5）四维张量"><span class="toc_mobile_items-number">5.5.</span> <span class="toc_mobile_items-text">（5）四维张量</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#索引和切片"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">索引和切片</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#维度变换"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">维度变换</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（1）视图"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text">（1）视图</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（2）增删维度"><span class="toc_mobile_items-number">7.2.</span> <span class="toc_mobile_items-text">（2）增删维度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（3）交换维度"><span class="toc_mobile_items-number">7.3.</span> <span class="toc_mobile_items-text">（3）交换维度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#（3）复制数据"><span class="toc_mobile_items-number">7.4.</span> <span class="toc_mobile_items-text">（3）复制数据</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Broadcasting"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">Broadcasting</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数学运算"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text">数学运算</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据类型"><span class="toc-number">1.</span> <span class="toc-text">数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）数值类型"><span class="toc-number">1.1.</span> <span class="toc-text">（1）数值类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）字符串类型"><span class="toc-number">1.2.</span> <span class="toc-text">（2）字符串类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）布尔类型"><span class="toc-number">1.3.</span> <span class="toc-text">（3）布尔类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数值精度"><span class="toc-number">2.</span> <span class="toc-text">数值精度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#待优化张量"><span class="toc-number">3.</span> <span class="toc-text">待优化张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建张量"><span class="toc-number">4.</span> <span class="toc-text">创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）-从数组、列表对象创建"><span class="toc-number">4.1.</span> <span class="toc-text">（1） 从数组、列表对象创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）创建全0或全1张量"><span class="toc-number">4.2.</span> <span class="toc-text">（2）创建全0或全1张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）创建自定义数值张量"><span class="toc-number">4.3.</span> <span class="toc-text">（3）创建自定义数值张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（4）创建已知分布的张量"><span class="toc-number">4.4.</span> <span class="toc-text">（4）创建已知分布的张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（5）创建序列"><span class="toc-number">4.5.</span> <span class="toc-text">（5）创建序列</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#张量的典型应用"><span class="toc-number">5.</span> <span class="toc-text">张量的典型应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）标量"><span class="toc-number">5.1.</span> <span class="toc-text">（1）标量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）向量"><span class="toc-number">5.2.</span> <span class="toc-text">（2）向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）矩阵"><span class="toc-number">5.3.</span> <span class="toc-text">（3）矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（4）三维张量"><span class="toc-number">5.4.</span> <span class="toc-text">（4）三维张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（5）四维张量"><span class="toc-number">5.5.</span> <span class="toc-text">（5）四维张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#索引和切片"><span class="toc-number">6.</span> <span class="toc-text">索引和切片</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#维度变换"><span class="toc-number">7.</span> <span class="toc-text">维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）视图"><span class="toc-number">7.1.</span> <span class="toc-text">（1）视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）增删维度"><span class="toc-number">7.2.</span> <span class="toc-text">（2）增删维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）交换维度"><span class="toc-number">7.3.</span> <span class="toc-text">（3）交换维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）复制数据"><span class="toc-number">7.4.</span> <span class="toc-text">（3）复制数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Broadcasting"><span class="toc-number">8.</span> <span class="toc-text">Broadcasting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数学运算"><span class="toc-number">9.</span> <span class="toc-text">数学运算</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://s2.ax1x.com/2020/02/27/3aTBOx.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Tensorflow基础</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-02-27<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-02-27</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><p>《Tensorflow深度学习》unit4学习记录</p>
<ol>
<li>Tensorflow内部数据保存在张量对象上，所有OP基于张量对象Tensor进行</li>
<li>复杂的神经网络算法本质上就是各种张量相乘、相加等基本运算操作的组合</li>
</ol>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>TensorFlow 中的基本数据类型，包含数值类型、字符串类型和布尔类型</p>
<h3 id="（1）数值类型"><a href="#（1）数值类型" class="headerlink" title="（1）数值类型"></a>（1）数值类型</h3><p>张量是TensorFlow的主要数据载体，根据维度数来区分</p>
<ol>
<li>标量：单个实数，Dimension = 0, shape = []</li>
<li>向量：n个实数的有序集合，Dimension = 1, shape = [n]</li>
<li>矩阵：[[2,3,4],[1,3,5]], Dimension = 2, shape = [n,m]</li>
<li>所有dimension>2的数组统称为张量，张量的每个维度作轴Axis，每个维度一般代表具体物理含义。</li>
</ol>
<p><strong>一般为表达方便，把标量、向量、矩阵统称为张量，不作区分</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建标量</span></span><br><span class="line">x = <span class="number">1.2</span> <span class="comment">#python语言创建</span></span><br><span class="line">xx = tf.constant(<span class="number">1.2</span>) <span class="comment">#TF方式创建,正确</span></span><br><span class="line">print(type(x), type(xx), tf.is_tensor(xx))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建向量</span></span><br><span class="line"><span class="comment"># 向量的定义须通过List 容器传给tf.constant()函数</span></span><br><span class="line">a = tf.constant([<span class="number">1.2</span>])</span><br><span class="line">print(a, a.shape)</span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">b = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(b, b.shape)</span><br><span class="line"><span class="comment"># 创建三维张量</span></span><br><span class="line">c = tf.constant([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]])</span><br><span class="line">print(c, c.shape)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">class</span> '<span class="attr">float</span>'></span></span> <span class="xml"><span class="tag"><<span class="name">class</span> '<span class="attr">tensorflow.python.framework.ops.EagerTensor</span>'></span></span> True</span><br><span class="line">tf.Tensor([1.2], shape=(1,), dtype=float32) (1,)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[1 2]</span><br><span class="line"> [3 4]], shape=(2, 2), dtype=int32) (2, 2)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[[1 2]</span><br><span class="line">  [3 4]]</span><br><span class="line"></span><br><span class="line"> [[5 6]</span><br><span class="line">  [7 8]]], shape=(2, 2, 2), dtype=int32) (2, 2, 2)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 打印张量的相关信息</span></span><br><span class="line">x = tf.constant([<span class="number">1</span>,<span class="number">2.</span>,<span class="number">3.3</span>])</span><br><span class="line">print(x) <span class="comment">#x</span></span><br><span class="line"><span class="comment"># id是TF中内部索引对象编号</span></span><br><span class="line"><span class="comment"># numpy()返回Numpy.appray类型数据，方便到处数据道系统其他模块</span></span><br><span class="line">x.numpy() <span class="comment">#将张量数据导出为numpy数组格式</span></span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">tf.Tensor([1.  2.  3.3], shape=(3,), dtype=float32)</span><br><span class="line"></span><br><span class="line">array([1. , 2. , 3.3], dtype=float32)</span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（2）字符串类型"><a href="#（2）字符串类型" class="headerlink" title="（2）字符串类型"></a>（2）字符串类型</h3><p>支持<strong>String类型</strong>，在表示图片数据时，可以先记录图片的路径字符串，再通过预处理函数根据路径读取图片张量。</p>
<ul>
<li>在tf.strings 模块中，提供了常见的字符串类型的工具函数，如小写化lower()、拼接join()、长度length()、切分split()等</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="string">'Hello, Deep Learning.'</span>)</span><br><span class="line">tf.strings.lower(a)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">6,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">string,</span> <span class="attr">numpy</span>=<span class="string">b</span>'<span class="attr">hello</span>, <span class="attr">deep</span> <span class="attr">learning.</span>'></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（3）布尔类型"><a href="#（3）布尔类型" class="headerlink" title="（3）布尔类型"></a>（3）布尔类型</h3><p>TensorFlow 的布尔类型和Python 语言的布尔类型并不等价，不能通用</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#标量</span></span><br><span class="line">a = tf.constant(<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 向量</span></span><br><span class="line">b = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line">print(a <span class="keyword">is</span> <span class="literal">True</span>) <span class="comment"># TF 布尔类型张量与python 布尔类型比较 不等价</span></span><br><span class="line">a == <span class="literal">True</span> <span class="comment"># 仅数值比较</span></span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">False</span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">10,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">bool,</span> <span class="attr">numpy</span>=<span class="string">True</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h2 id="数值精度"><a href="#数值精度" class="headerlink" title="数值精度"></a>数值精度</h2><ul>
<li>数值类型张量可保存为不同字节长度的精度</li>
<li>如浮点数3.14 既可以保存为16 位(Bit)长度，也可以保存为32位甚至64位的精度。</li>
<li>位越长，精度越高，同时占用的内存空间也就越大。</li>
<li>常用的精度类型有tf.int16、tf.int32、tf.int64、tf.float16、tf.float32、tf.float64 等，其中tf.float64 即为tf.double。</li>
</ul>
<hr>
<ul>
<li>精度过低导致数组溢出，一般使用tf.int32、tf.int64精度。</li>
<li>对于浮点数，高精度的张量可以表示更精准的数据，例如采用tf.float32 精度保存π时，实际保存的数据为3.1415927。</li>
</ul>
<hr>
<p><strong>对于大部分深度学习算法，一般使用tf.int32 和tf.float32 可满足大部分场合的运算精度要求，部分对精度要求较高的算法，如强化学习某些算法，可以选择使用tf.int64 和tf.float64 精度保存张量。</strong></p>
<ul>
<li>需要注意的是，Numpy 浮点数数组默认使用64 位精度保存数据，转换到Tensor 类型时精度为tf.float64，可以在需要的时候将其转换为tf.float32 类</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">print(tf.constant(<span class="number">123456789</span>, dtype=tf.int16))</span><br><span class="line">tf.constant(<span class="number">123456789</span>, dtype=tf.int32)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">tf.Tensor(-13035, shape=(), dtype=int16)</span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">12,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">123456789</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">tf.constant(np.pi, dtype=tf.float32)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">13,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">3.1415927</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<p><strong>精度读取与类型转换</strong><br></p>
<ul>
<li>对于某些只能处理指定精度类型的运算操作，需要提前检验输入张量的精度类型，并将不符合要求的张量进行类型转换。</li>
<li>进行类型转换时，需要保证转换操作的合法性，例如将高精度的张量转换为低精度的张量时，可能发生数据溢出隐患。</li>
<li><strong>布尔类型与整型之间相互转换也合法</strong></li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.constant(np.pi, dtype=tf.float16)</span><br><span class="line">print(<span class="string">'before:'</span>,a.dtype)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> a.dtype != tf.float32:</span><br><span class="line">    a = tf.cast(a, tf.float32)</span><br><span class="line">print(<span class="string">'after:'</span>,a.dtype)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">before: <span class="xml"><span class="tag"><<span class="name">dtype:</span> '<span class="attr">float16</span>'></span></span></span><br><span class="line">after: <span class="xml"><span class="tag"><<span class="name">dtype:</span> '<span class="attr">float32</span>'></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 不合法转换</span></span><br><span class="line">a = tf.constant(<span class="number">123456789</span>, dtype=tf.int32)</span><br><span class="line">tf.cast(a, tf.int16)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">17,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">int16,</span> <span class="attr">numpy</span>=<span class="string">-13035</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">tf.cast(a, tf.int32)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">19,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([1,</span> <span class="attr">0</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 在TensorFlow 中，将非0数字都视为True</span></span><br><span class="line">a = tf.constant([<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tf.cast(a, tf.bool)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">21,</span> <span class="attr">shape</span>=<span class="string">(4,),</span> <span class="attr">dtype</span>=<span class="string">bool,</span> <span class="attr">numpy</span>=<span class="string">array([</span> <span class="attr">True</span>, <span class="attr">False</span>,  <span class="attr">True</span>,  <span class="attr">True</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h2 id="待优化张量"><a href="#待优化张量" class="headerlink" title="待优化张量"></a>待优化张量</h2><ul>
<li>为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量，TensorFlow增加了一种专门的数据类型来支持梯度信息的记录：<strong>tf.Variable</strong></li>
<li>tf.Variable 类型在普通的张量类型基础上添加了name，trainable等属性来支持计算图的构建。</li>
<li>由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入𝑿，不需要通过tf.Variable 封装；相反，<strong>对于需要计算梯度并优化的张量，如神经网络层的𝑾和𝒃，需要通过tf.Variable 包裹以便TensorFlow 跟踪相关梯度信息。</strong></li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line">a = tf.constant([<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 创建TF 张量</span></span><br><span class="line">aa = tf.Variable(a) <span class="comment"># 转换为Variable 类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接创建Variable张量</span></span><br><span class="line">b = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) </span><br><span class="line"></span><br><span class="line">aa.name, aa.trainable <span class="comment"># Variable 类型张量的属性</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">('Variable:0', True)</span><br></pre></td></tr></tbody></table></figure></div>

<ul>
<li>name 属性用于命名计算图中的变量，这套命名体系是TensorFlow 内部维护的，一般不需要用户关注</li>
<li>trainable属性表征当前张量是否需要被优化，创建Variable 对象时是默认启用优化标志，可以设置trainable=False 来设置张量不需要优化。</li>
<li><strong>待优化张量可视为普通张量的特殊类型，普通张量其实也可以通过GradientTape.watch()方法临时加入跟踪梯度信息的列表，从而支持自动求导功能。</strong></li>
</ul>
<h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>在 TensorFlow 中，可以通过多种方式创建张量，如从<strong>Python 列表对象</strong>创建，从<strong>Numpy 数组</strong>创建，或者创建<strong>采样自某种已知分布的张量</strong>等。</p>
<h3 id="（1）-从数组、列表对象创建"><a href="#（1）-从数组、列表对象创建" class="headerlink" title="（1） 从数组、列表对象创建"></a>（1） 从数组、列表对象创建</h3><ul>
<li>tf.constant()</li>
<li>tf.convert_to_tensor()</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.convert_to_tensor(np.array([[<span class="number">1</span>,<span class="number">2.</span>],[<span class="number">3</span>,<span class="number">4</span>]]))</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">37,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float64,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[1., 2.],</span></span><br><span class="line"><span class="xml">       [3., 4.]])></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.convert_to_tensor([<span class="number">1</span>,<span class="number">2.</span>])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">38,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([1.,</span> <span class="attr">2.</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.constant([<span class="number">1</span>,<span class="number">2.</span>]) <span class="comment">#同理</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">39,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([1.,</span> <span class="attr">2.</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（2）创建全0或全1张量"><a href="#（2）创建全0或全1张量" class="headerlink" title="（2）创建全0或全1张量"></a>（2）创建全0或全1张量</h3><ul>
<li>常用张量初始化手段 tf.zeros/ones()</li>
<li>tf.zeros_like, tf.ones_like 可以方便地新建与某个张量shape一致，且内容为全0或全1的张量</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.zeros([]),tf.ones([])<span class="comment">#标量</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">(<span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">40,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">0.0</span>></span></span>,</span><br><span class="line"> <span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">41,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">1.0</span>></span></span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.zeros([<span class="number">1</span>]),tf.ones([<span class="number">1</span>])<span class="comment">#向量</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">(<span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">44,</span> <span class="attr">shape</span>=<span class="string">(1,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([0.],</span> <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span>,</span><br><span class="line"> <span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">47,</span> <span class="attr">shape</span>=<span class="string">(1,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([1.],</span> <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.zeros([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment">#矩阵</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.zeros_like(a)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">54,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[0., 0.],</span></span><br><span class="line"><span class="xml">       [0., 0.]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.ones_like(a)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">57,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[1., 1.],</span></span><br><span class="line"><span class="xml">       [1., 1.]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（3）创建自定义数值张量"><a href="#（3）创建自定义数值张量" class="headerlink" title="（3）创建自定义数值张量"></a>（3）创建自定义数值张量</h3><ul>
<li>tf.fill(shape, value)</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">print( tf.fill([], <span class="number">-1</span>) )</span><br><span class="line">print( tf.fill([<span class="number">1</span>], <span class="number">-1</span>) )</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">tf.Tensor(-1, shape=(), dtype=int32)</span><br><span class="line">tf.Tensor([-1], shape=(1,), dtype=int32)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.fill([<span class="number">2</span>,<span class="number">2</span>], <span class="number">99</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">75,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[99, 99],</span></span><br><span class="line"><span class="xml">       [99, 99]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（4）创建已知分布的张量"><a href="#（4）创建已知分布的张量" class="headerlink" title="（4）创建已知分布的张量"></a>（4）创建已知分布的张量</h3><p>正态分布(Normal Distribution，或Gaussian Distribution)和均匀分布(Uniform Distribution)是最常见的分布之一，创建采样自这2种分布的张量非常有用。<br><br>比如<strong>在卷积神经网络中，卷积核张量𝑾初始化为正态分布有利于网络的训练</strong>；在对抗生成网络中，隐藏变量𝒛一般采样自均匀分布。</p>
<ul>
<li>tf.random.normal(shape, mean=0.0, stddev=1.0) 正太分布</li>
<li>tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)创建采样自[minval, maxval)区间的<strong>均匀分布</strong>的张量</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.random.normal([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 创建标准正态分布的张量 均值为0，标准差为1</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">81,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[-0.24957317,  3.313951  ],</span></span><br><span class="line"><span class="xml">       [ 1.585423  , -1.4045318 ]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.random.normal([<span class="number">2</span>,<span class="number">2</span>], mean=<span class="number">1</span>,stddev=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">87,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[ 2.121697  ,  0.4550423 ],</span></span><br><span class="line"><span class="xml">       [-0.30552602,  1.1206111 ]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>],maxval=<span class="number">10</span>) <span class="comment">#[0,10)</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">101,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[2.0133781, 3.5010254],</span></span><br><span class="line"><span class="xml">       [4.711748 , 9.191704 ]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 如果需要均匀采样整形类型的数据，必须指定采样区间的最大值maxval 参数</span></span><br><span class="line"><span class="comment"># 同时指定数据类型为tf.int*型</span></span><br><span class="line">tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>],maxval=<span class="number">100</span>,dtype=tf.int32)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">105,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[74,  3],</span></span><br><span class="line"><span class="xml">       [22, 10]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（5）创建序列"><a href="#（5）创建序列" class="headerlink" title="（5）创建序列"></a>（5）创建序列</h3><p>在<strong>循环计算或者对张量进行索引</strong>时，经常需要创建一段连续的整型序列，可以通过tf.range()函数实现。</p>
<ul>
<li>tf.range(limit, delta=1)可以创建[0, limit)之间，步长为delta 的整型序列。</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.range(<span class="number">10</span>,delta = <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">113,</span> <span class="attr">shape</span>=<span class="string">(5,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([0,</span> <span class="attr">2</span>, <span class="attr">4</span>, <span class="attr">6</span>, <span class="attr">8</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.range(<span class="number">1</span>,<span class="number">10</span>,delta = <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">117,</span> <span class="attr">shape</span>=<span class="string">(5,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([1,</span> <span class="attr">3</span>, <span class="attr">5</span>, <span class="attr">7</span>, <span class="attr">9</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h2 id="张量的典型应用"><a href="#张量的典型应用" class="headerlink" title="张量的典型应用"></a>张量的典型应用</h2><ul>
<li>在TensorFlow 中，标量最容易理解，它就是一个简单的数字，维度数为0，shape为[]。</li>
<li>标量的一些典型用途是误差值的表示、各种测量指标的表示，比如准确度(Accuracy，简称acc)，精度(Precision)和召回率(Recall)等。</li>
</ul>
<h3 id="（1）标量"><a href="#（1）标量" class="headerlink" title="（1）标量"></a>（1）标量</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">out = tf.random.uniform([<span class="number">4</span>,<span class="number">10</span>]) <span class="comment">#随机模拟网络输出，均匀分布</span></span><br><span class="line">y = tf.constant([<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>]) <span class="comment"># 随机构造样本真实标签，向量</span></span><br><span class="line">y = tf.one_hot(y, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line">loss = tf.keras.losses.mse(y, out) <span class="comment"># 计算每个样本的MSE</span></span><br><span class="line">loss = tf.reduce_mean(loss) <span class="comment"># 平均MSE,loss 应是标量</span></span><br><span class="line">print(loss)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">tf.Tensor(0.363449, shape=(), dtype=float32)</span><br></pre></td></tr></tbody></table></figure></div>


<p>取误差的均值作为当前Batch 的误差，它是一个标量</p>
<h3 id="（2）向量"><a href="#（2）向量" class="headerlink" title="（2）向量"></a>（2）向量</h3><ul>
<li>向量是一种非常常见的数据载体，如在全连接层和卷积神经网络层中，偏置张量𝒃就使用向量来表示。</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># z=wx,模拟获得激活函数的输入z</span></span><br><span class="line">z = tf.random.normal([<span class="number">4</span>,<span class="number">2</span>]) <span class="comment">#正太分布</span></span><br><span class="line">b = tf.zeros([<span class="number">2</span>]) <span class="comment"># 创建偏置向量[1,2]</span></span><br><span class="line">z = z + b <span class="comment"># 累加上偏置向量 shape 为[4,2]的𝒛和shape 为[2]的𝒃张量可以直接相加</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>通过高层接口类Dense()方式创建的网络层，张量𝑾和𝒃存储在类的内部，由类自动创建并管理。<br><br>可以通过全连接层的bias 成员变量查看偏置变量𝒃，例如创建输入节点数为4，输出节点数为3的线性层网络，那么它的偏置向量b的长度应为3，实现如下：<br><br>可以看到，类的偏置成员bias 为长度为3 的向量，初始化为全0，这也是偏置𝒃的默认初始化方案。同时偏置向量𝒃的类型为Variable，这是因为𝑾和𝒃都是待优化参数。<br></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fc = tf.keras.layers.Dense(<span class="number">3</span>) <span class="comment"># 创建一层Wx+b，输出节点为3</span></span><br><span class="line"><span class="comment"># 通过build 函数创建W,b 张量，输入节点为4</span></span><br><span class="line">fc.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">fc.bias</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Variable</span> '<span class="attr">bias:0</span>' <span class="attr">shape</span>=<span class="string">(3,)</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([0.,</span> <span class="attr">0.</span>, <span class="attr">0.</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（3）矩阵"><a href="#（3）矩阵" class="headerlink" title="（3）矩阵"></a>（3）矩阵</h3><ul>
<li>全连接层的批量输入张量𝑿的形状为[𝑏, 𝑑in]</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>]) <span class="comment"># 2 个样本，特征长度为4 的张量</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>令全连接层的输出节点数为3，则它的权值张量𝑾的shape 为[4,3]，我们利用张量𝑿、𝑾和向量𝒃可以直接实现一个网络层</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">w = tf.ones([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.zeros([<span class="number">3</span>])</span><br><span class="line">o = x @ w + b</span><br></pre></td></tr></tbody></table></figure></div>

<p>其中𝑿和𝑾张量均是矩阵，上述代码实现了一个线性变换的网络层，激活函数为空。一般地，𝜎(𝑿@𝑾 + 𝒃)网络层称为全连接层，在TensorFlow 中可以通过Dense类直接实现，特别地，<strong>当激活函数𝜎为空时，全连接层也称为线性层</strong>。<br><br>通过Dense 类创建输入4 个节点，输出3个节点的网络层，并通过全连接层的kernel 成员名查看其权值矩阵𝑾。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fc = tf.keras.layers.Dense(<span class="number">3</span>)</span><br><span class="line">fc.build(input_shape = (<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">fc.kernel</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Variable</span> '<span class="attr">kernel:0</span>' <span class="attr">shape</span>=<span class="string">(4,</span> <span class="attr">3</span>) <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[-0.72585213,  0.02908283,  0.69765556],</span></span><br><span class="line"><span class="xml">       [ 0.18113148, -0.74577487, -0.78170925],</span></span><br><span class="line"><span class="xml">       [-0.04508269,  0.8460268 , -0.76969826],</span></span><br><span class="line"><span class="xml">       [-0.40836567, -0.5030982 ,  0.83116066]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（4）三维张量"><a href="#（4）三维张量" class="headerlink" title="（4）三维张量"></a>（4）三维张量</h3><ul>
<li>三维的张量一个典型应用是表示<strong>序列信号</strong>，它的格式是<strong>𝑿 = [𝑏, sequence len, feature len]</strong></li>
<li>其中𝑏表示序列信号的数量，sequence len 表示序列信号在时间维度上的采样点数或步数，feature len 表示每个点的特征长度。</li>
</ul>
<p>例：<br><br>考虑<strong>NLP中句子的表示</strong>，如评价句子的是否为正面情绪的情感分类任务网络。<br><br>为了能够方便字符串被神经网络处理，一般将单词通过嵌入层(Embedding Layer)编码为固定长度的向量，比如“a”编码为某个长度3 的向量，那么2个等长(单词数量为5)的句子序列可以表示为shape 为[2,5,3]的3 维张量。<br><br>IMDB 数据集来演示如何表示句子，代码如下：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">(x_train,y_train),(x_test,y_test) = tf.keras.datasets.imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz</span><br><span class="line">17465344/17464789 [==============================] - 9s 1us/step</span><br></pre></td></tr></tbody></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 将句子填充、截断为等长80 个单词的句子</span></span><br><span class="line">x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=<span class="number">80</span>)</span><br><span class="line">x_train.shape</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">(25000, 80)</span><br></pre></td></tr></tbody></table></figure></div>

<p>25000个句子，80个单词，每个单词使用数字编码方式表示。<br><br>我们通过layers.Embedding层将<strong>数字编码</strong>的单词转换为<strong>长度的100个词向量</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 创建词向量Embedding 层类</span></span><br><span class="line">embedding = tf.keras.layers.Embedding(<span class="number">10000</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 将数字编码的单词转换为词向量</span></span><br><span class="line">out = embedding(x_train)</span><br><span class="line">out.shape</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([25000, 80, 100])</span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（5）四维张量"><a href="#（5）四维张量" class="headerlink" title="（5）四维张量"></a>（5）四维张量</h3><ul>
<li>大于四维的张量一般应用的比较少，如在元学习(Meta Learning)中会采用五维的张量表示方法，理解方法与三、四维张量类似。</li>
<li><strong>四维张量在卷积神经网络中应用非常广泛，它用于保存特征图(Feature maps)数据，格式一般定义为[𝑏, ℎ, w, 𝑐]</strong></li>
<li>其中𝑏表示输入样本的数量，ℎ/ w分别表示特征图的高/宽，𝑐表示特征图的通道数，部分深度学习框架也会使用[𝑏, 𝑐, ℎ, w]格式的特征图张量，例如PyTorch。</li>
<li>图片数据是特征图的一种，对于含有RGB 3 个通道的彩色图片，每张图片包含了ℎ行w列像素点，每个点需要3个数值表示RGB 通道的颜色强度，因此一张图片可以表示为[ℎ, w, 3]。</li>
</ul>
<p>神经网络中一般<strong>并行计算多个输入以提高计算效率</strong>，故𝑏张图片的张量可表示为<br>[𝑏, ℎ, w, 3]</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 创建32x32 的彩色图片输入，个数为4</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 创建卷积神经网络 输出为16</span></span><br><span class="line">layer = tf.keras.layers.Conv2D(<span class="number">16</span>,kernel_size=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">out = layer(x) </span><br><span class="line"><span class="comment"># 输出大小</span></span><br><span class="line">out.shape</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([4, 30, 30, 16])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 其中卷积核张量也是4 维张量，可以通过kernel 成员变量访问</span></span><br><span class="line">layer.kernel.shape</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([3, 3, 3, 16])</span><br></pre></td></tr></tbody></table></figure></div>

<h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><p>通过索引与切片操作可以提取张量的部分数据，它们的使用频率非常高</p>
<p><strong>索引</strong></p>
<ul>
<li>在 TensorFlow 中，支持基本的[𝑖][𝑗] ⋯标准索引方式</li>
<li>也支持通过逗号分隔索引号的索引方式</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[<span class="number">0</span>] <span class="comment">#第一张图片</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">297,</span> <span class="attr">shape</span>=<span class="string">(32,</span> <span class="attr">32</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[ 0.01287534, -0.33164835,  0.39746076],</span></span><br><span class="line"><span class="xml">        [ 0.45679998,  0.21221888,  1.1897283 ],</span></span><br><span class="line"><span class="xml">        [-1.0795164 ,  1.0956312 ,  1.2745731 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 1.0313561 ,  0.00735768, -0.13328278],</span></span><br><span class="line"><span class="xml">        [-0.7328737 , -0.47847667, -1.1222742 ],</span></span><br><span class="line"><span class="xml">        [-0.562944  , -0.6294002 , -0.536716  ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.12896156,  0.20843787,  0.48825002],</span></span><br><span class="line"><span class="xml">        [ 0.46369797, -1.2704887 ,  0.9649579 ],</span></span><br><span class="line"><span class="xml">        [ 0.5921016 , -0.96782595, -0.5078653 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 0.92102367,  1.1205709 , -0.67090195],</span></span><br><span class="line"><span class="xml">        [-1.7881484 ,  0.22966936,  0.44847772],</span></span><br><span class="line"><span class="xml">        [ 0.5319272 , -0.50274354, -0.45568085]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.95825106, -0.13270964,  1.9421529 ],</span></span><br><span class="line"><span class="xml">        [-1.0019497 , -1.5856102 , -0.97071743],</span></span><br><span class="line"><span class="xml">        [-0.54386234, -1.3851955 , -0.14460503],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 1.7483561 ,  0.29159114,  2.3191211 ],</span></span><br><span class="line"><span class="xml">        [ 1.1493669 , -1.1993392 ,  0.67197794],</span></span><br><span class="line"><span class="xml">        [ 0.04841843, -1.2324219 ,  0.73856485]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       ...,</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.68325275, -0.47775906,  0.54074013],</span></span><br><span class="line"><span class="xml">        [-0.41808888,  0.23198034, -1.1096005 ],</span></span><br><span class="line"><span class="xml">        [ 1.0632559 ,  1.0266107 ,  1.4928274 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 2.4625192 , -0.2885487 ,  1.1911327 ],</span></span><br><span class="line"><span class="xml">        [-1.5033373 , -0.7605944 ,  2.2961745 ],</span></span><br><span class="line"><span class="xml">        [-0.95298505, -0.28505468,  0.7217069 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ 1.0398324 ,  0.45077163,  0.39368516],</span></span><br><span class="line"><span class="xml">        [-1.1132933 , -0.25639424, -1.2910725 ],</span></span><br><span class="line"><span class="xml">        [ 1.0095906 ,  0.99504983,  0.7486682 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.20738599,  0.04625274,  1.583734  ],</span></span><br><span class="line"><span class="xml">        [-0.05181801, -0.2961739 ,  0.7241903 ],</span></span><br><span class="line"><span class="xml">        [ 1.1167142 , -0.47221646, -1.785053  ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ 0.6377164 , -1.3216083 ,  0.52627456],</span></span><br><span class="line"><span class="xml">        [-1.5968251 ,  2.0323265 , -1.0769714 ],</span></span><br><span class="line"><span class="xml">        [ 0.25630292, -1.1222397 , -1.1831323 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 0.6970177 ,  0.7082761 , -0.78703374],</span></span><br><span class="line"><span class="xml">        [ 0.16549316, -0.04083331, -0.9949377 ],</span></span><br><span class="line"><span class="xml">        [ 1.1936904 ,  0.1333852 , -1.0029237 ]]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[<span class="number">0</span>][<span class="number">1</span>] <span class="comment">#图片1第2行</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">305,</span> <span class="attr">shape</span>=<span class="string">(32,</span> <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[-1.2896156e-01,  2.0843787e-01,  4.8825002e-01],</span></span><br><span class="line"><span class="xml">       [ 4.6369797e-01, -1.2704887e+00,  9.6495789e-01],</span></span><br><span class="line"><span class="xml">       [ 5.9210157e-01, -9.6782595e-01, -5.0786531e-01],</span></span><br><span class="line"><span class="xml">       [ 7.7936709e-01,  4.4635782e-01, -9.3138492e-01],</span></span><br><span class="line"><span class="xml">       [ 1.3500343e-01,  7.6426953e-01, -5.9938073e-01],</span></span><br><span class="line"><span class="xml">       [ 4.9844036e-01, -3.4117934e-01, -2.3618491e-01],</span></span><br><span class="line"><span class="xml">       [ 4.0358964e-01, -4.9796373e-01,  5.3907746e-01],</span></span><br><span class="line"><span class="xml">       [-1.8684142e+00, -1.1775721e+00,  3.5153952e-01],</span></span><br><span class="line"><span class="xml">       [ 6.8103689e-01, -1.3204192e+00,  1.5827950e+00],</span></span><br><span class="line"><span class="xml">       [ 1.6188347e+00, -1.3795718e+00, -4.8911616e-01],</span></span><br><span class="line"><span class="xml">       [-6.0763979e-01,  1.4898417e+00,  3.7245435e-04],</span></span><br><span class="line"><span class="xml">       [-1.8180510e-01,  3.2856616e-01, -1.2196579e+00],</span></span><br><span class="line"><span class="xml">       [-1.2901436e+00,  1.9346505e-01, -6.7549396e-01],</span></span><br><span class="line"><span class="xml">       [-1.4701049e+00,  1.6117885e+00, -1.7378354e+00],</span></span><br><span class="line"><span class="xml">       [ 5.6874659e-03, -1.5885623e+00,  8.6967957e-01],</span></span><br><span class="line"><span class="xml">       [ 1.0910570e-01,  6.0375035e-01, -2.4967529e-01],</span></span><br><span class="line"><span class="xml">       [ 7.5067736e-02,  7.9225957e-01, -3.2183570e-01],</span></span><br><span class="line"><span class="xml">       [ 1.2418184e+00, -2.3592411e-01, -3.6787802e-01],</span></span><br><span class="line"><span class="xml">       [ 8.6481667e-01,  4.8228177e-01, -6.4589536e-01],</span></span><br><span class="line"><span class="xml">       [ 4.5633847e-01, -5.5683923e-01, -3.0189914e-01],</span></span><br><span class="line"><span class="xml">       [-1.4300978e+00, -1.7237781e-01, -6.7511916e-01],</span></span><br><span class="line"><span class="xml">       [ 2.5924581e-01, -1.5654175e+00, -7.6849610e-02],</span></span><br><span class="line"><span class="xml">       [ 3.0996805e-01,  5.8938688e-01, -7.7999765e-01],</span></span><br><span class="line"><span class="xml">       [-6.1211109e-01,  1.7579930e+00, -5.6003171e-01],</span></span><br><span class="line"><span class="xml">       [-1.2746811e+00,  6.1794025e-01, -5.0755744e-03],</span></span><br><span class="line"><span class="xml">       [-1.3106546e-01,  7.8319013e-01,  3.3061635e-01],</span></span><br><span class="line"><span class="xml">       [-9.7331005e-01,  7.3418516e-01, -7.9657125e-01],</span></span><br><span class="line"><span class="xml">       [ 3.8233906e-01, -4.7034577e-01, -3.1503937e-01],</span></span><br><span class="line"><span class="xml">       [-7.6032549e-01,  1.5978434e+00, -2.8515723e-01],</span></span><br><span class="line"><span class="xml">       [ 9.2102367e-01,  1.1205709e+00, -6.7090195e-01],</span></span><br><span class="line"><span class="xml">       [-1.7881484e+00,  2.2966936e-01,  4.4847772e-01],</span></span><br><span class="line"><span class="xml">       [ 5.3192723e-01, -5.0274354e-01, -4.5568085e-01]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 第1 张图片，第2 行，第3 列</span></span><br><span class="line"><span class="comment">#x[0][1][2] </span></span><br><span class="line">x[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">337,</span> <span class="attr">shape</span>=<span class="string">(3,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([</span> <span class="attr">0.5921016</span> , <span class="attr">-0.96782595</span>, <span class="attr">-0.5078653</span> ], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 第 3 张图片，第2 行，第1 列的像素，B 通道(第2 个通道)颜色强度</span></span><br><span class="line">x[<span class="number">2</span>][<span class="number">1</span>][<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">333,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">0.5940293</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<p><strong>切片</strong></p>
<ul>
<li>start: end: step 区间[start,end)</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 全部省略时即为::，表示从最开始读取到最末尾，步长为1</span></span><br><span class="line">x[<span class="number">0</span>,::]</span><br><span class="line"><span class="comment">#表示读取第1 张图片的所有行==x[0],为了更加简洁，::可以简写为单个冒号:</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">353,</span> <span class="attr">shape</span>=<span class="string">(32,</span> <span class="attr">32</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[ 0.01287534, -0.33164835,  0.39746076],</span></span><br><span class="line"><span class="xml">        [ 0.45679998,  0.21221888,  1.1897283 ],</span></span><br><span class="line"><span class="xml">        [-1.0795164 ,  1.0956312 ,  1.2745731 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 1.0313561 ,  0.00735768, -0.13328278],</span></span><br><span class="line"><span class="xml">        [-0.7328737 , -0.47847667, -1.1222742 ],</span></span><br><span class="line"><span class="xml">        [-0.562944  , -0.6294002 , -0.536716  ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.12896156,  0.20843787,  0.48825002],</span></span><br><span class="line"><span class="xml">        [ 0.46369797, -1.2704887 ,  0.9649579 ],</span></span><br><span class="line"><span class="xml">        [ 0.5921016 , -0.96782595, -0.5078653 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 0.92102367,  1.1205709 , -0.67090195],</span></span><br><span class="line"><span class="xml">        [-1.7881484 ,  0.22966936,  0.44847772],</span></span><br><span class="line"><span class="xml">        [ 0.5319272 , -0.50274354, -0.45568085]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.95825106, -0.13270964,  1.9421529 ],</span></span><br><span class="line"><span class="xml">        [-1.0019497 , -1.5856102 , -0.97071743],</span></span><br><span class="line"><span class="xml">        [-0.54386234, -1.3851955 , -0.14460503],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 1.7483561 ,  0.29159114,  2.3191211 ],</span></span><br><span class="line"><span class="xml">        [ 1.1493669 , -1.1993392 ,  0.67197794],</span></span><br><span class="line"><span class="xml">        [ 0.04841843, -1.2324219 ,  0.73856485]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       ...,</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.68325275, -0.47775906,  0.54074013],</span></span><br><span class="line"><span class="xml">        [-0.41808888,  0.23198034, -1.1096005 ],</span></span><br><span class="line"><span class="xml">        [ 1.0632559 ,  1.0266107 ,  1.4928274 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 2.4625192 , -0.2885487 ,  1.1911327 ],</span></span><br><span class="line"><span class="xml">        [-1.5033373 , -0.7605944 ,  2.2961745 ],</span></span><br><span class="line"><span class="xml">        [-0.95298505, -0.28505468,  0.7217069 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ 1.0398324 ,  0.45077163,  0.39368516],</span></span><br><span class="line"><span class="xml">        [-1.1132933 , -0.25639424, -1.2910725 ],</span></span><br><span class="line"><span class="xml">        [ 1.0095906 ,  0.99504983,  0.7486682 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.20738599,  0.04625274,  1.583734  ],</span></span><br><span class="line"><span class="xml">        [-0.05181801, -0.2961739 ,  0.7241903 ],</span></span><br><span class="line"><span class="xml">        [ 1.1167142 , -0.47221646, -1.785053  ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ 0.6377164 , -1.3216083 ,  0.52627456],</span></span><br><span class="line"><span class="xml">        [-1.5968251 ,  2.0323265 , -1.0769714 ],</span></span><br><span class="line"><span class="xml">        [ 0.25630292, -1.1222397 , -1.1831323 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ 0.6970177 ,  0.7082761 , -0.78703374],</span></span><br><span class="line"><span class="xml">        [ 0.16549316, -0.04083331, -0.9949377 ],</span></span><br><span class="line"><span class="xml">        [ 1.1936904 ,  0.1333852 , -1.0029237 ]]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[:,<span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>,:]</span><br><span class="line"><span class="comment"># 表示读取所有图片、隔行采样、隔列采样</span></span><br><span class="line"><span class="comment">#读取所有通道数据</span></span><br><span class="line"><span class="comment">#相当于在图片的高宽上各缩放至原来的50%</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">357,</span> <span class="attr">shape</span>=<span class="string">(4,</span> <span class="attr">14</span>, <span class="attr">14</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[ 1.2875339e-02, -3.3164835e-01,  3.9746076e-01],</span></span><br><span class="line"><span class="xml">         [-1.0795164e+00,  1.0956312e+00,  1.2745731e+00],</span></span><br><span class="line"><span class="xml">         [-3.6790553e-01,  3.1922927e-01, -1.5245589e+00],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br></pre></td></tr></tbody></table></figure></div>

<ul>
<li>start: end: ​step</li>
<li>start:end</li>
<li>start:</li>
<li>start::step</li>
<li>: end: ​step</li>
<li>:end</li>
<li>::step</li>
<li>::和:读取所有元素</li>
</ul>
<hr>
<ul>
<li>step可以为负数</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.range(<span class="number">9</span>)</span><br><span class="line">x[<span class="number">8</span>:<span class="number">0</span>:<span class="number">-1</span>] <span class="comment"># 从8 取到0，逆序，不包含0</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">369,</span> <span class="attr">shape</span>=<span class="string">(8,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([8,</span> <span class="attr">7</span>, <span class="attr">6</span>, <span class="attr">5</span>, <span class="attr">4</span>, <span class="attr">3</span>, <span class="attr">2</span>, <span class="attr">1</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[::<span class="number">-1</span>] <span class="comment"># 逆序全部元素</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">373,</span> <span class="attr">shape</span>=<span class="string">(9,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([8,</span> <span class="attr">7</span>, <span class="attr">6</span>, <span class="attr">5</span>, <span class="attr">4</span>, <span class="attr">3</span>, <span class="attr">2</span>, <span class="attr">1</span>, <span class="attr">0</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[::<span class="number">-2</span>] <span class="comment"># 逆序间隔采样</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">377,</span> <span class="attr">shape</span>=<span class="string">(5,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([8,</span> <span class="attr">6</span>, <span class="attr">4</span>, <span class="attr">2</span>, <span class="attr">0</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#读取每张图片的所有通道，其中行按着逆序隔行采样，列按着逆序隔行采样，实现如下：</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line">x[<span class="number">0</span>,::<span class="number">-2</span>,::<span class="number">-2</span>] <span class="comment"># 行、列逆序间隔采样</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">387,</span> <span class="attr">shape</span>=<span class="string">(16,</span> <span class="attr">16</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[-7.84064412e-01, -8.32194388e-01, -1.27412587e-01],</span></span><br><span class="line"><span class="xml">        [ 3.92509811e-02, -4.53706868e-02, -1.39303970e+00],</span></span><br><span class="line"><span class="xml">        [-6.41830444e-01,  1.27360332e+00, -3.28318417e-01],</span></span><br><span class="line"><span class="xml">        [ 3.76418978e-01,  1.29475987e+00, -2.63758612e+00],</span></span><br><span class="line"><span class="xml">        [-3.08787197e-01, -7.33098030e-01,  5.52425861e-01],</span></span><br><span class="line"><span class="xml">        [ 4.15446162e-02,  1.03238404e+00,  2.98625708e-01],</span></span><br><span class="line"><span class="xml">        [-4.11630124e-01, -2.32803613e-01,  8.30142558e-01],</span></span><br><span class="line"><span class="xml">        [-9.34224054e-02, -5.31632416e-02,  6.87848151e-01],</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[:,:,:,<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">391,</span> <span class="attr">shape</span>=<span class="string">(4,</span> <span class="attr">32</span>, <span class="attr">32</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[-0.55588394, -0.49143514,  0.91525185, ...,  1.244667  ,</span></span><br><span class="line"><span class="xml">          0.33508417,  1.4909495 ],</span></span><br><span class="line"><span class="xml">        [-0.3253711 ,  0.28604615,  0.05659833, ..., -1.849753  ,</span></span><br><span class="line"><span class="xml">          0.653694  , -0.08815705],</span></span><br><span class="line"><span class="xml">        [-0.06575333, -0.96254736, -0.6946633 , ..., -0.11086184,</span></span><br><span class="line"><span class="xml">         -0.47008434,  1.2145292 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.13497923,  0.6384983 ,  1.6137513 , ..., -1.496228  ,</span></span><br><span class="line"><span class="xml">          1.6484805 , -1.7958195 ],</span></span><br><span class="line"><span class="xml">        [ 0.74971485,  0.0996201 ,  0.19288296, ...,  0.39004284,</span></span><br><span class="line"><span class="xml">         -1.2539461 ,  0.00935514],</span></span><br><span class="line"><span class="xml">        [-0.04224875,  0.07937325, -0.26393986, ..., -0.04537069,</span></span><br><span class="line"><span class="xml">         -0.57464653, -0.8321944 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ 1.1046472 , -0.5517117 ,  0.7572501 , ..., -1.5693089 ,</span></span><br><span class="line"><span class="xml">          0.37573445, -0.38332534],</span></span><br><span class="line"><span class="xml">        [ 0.41919976, -0.27044266,  0.09086894, ..., -0.75913143,</span></span><br><span class="line"><span class="xml">          1.2493117 ,  0.14186034],</span></span><br><span class="line"><span class="xml">        [ 0.46797848,  0.6651568 , -0.43626037, ..., -0.03528596,</span></span><br><span class="line"><span class="xml">          2.161981  , -0.18836296],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.6833904 , -0.65184695, -1.1479295 , ...,  1.0325705 ,</span></span><br><span class="line"><span class="xml">          1.4662279 ,  1.3297852 ],</span></span><br><span class="line"><span class="xml">        [-1.1102995 ,  2.3700943 ,  0.78097135, ...,  0.8990006 ,</span></span><br><span class="line"><span class="xml">          1.2611197 , -0.3595686 ],</span></span><br><span class="line"><span class="xml">        [ 1.5362923 ,  0.5991842 , -0.00354334, ..., -0.07285233,</span></span><br><span class="line"><span class="xml">          0.45789108,  0.4289513 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-1.7803544 ,  1.4067653 ,  1.1593533 , ..., -0.30160835,</span></span><br><span class="line"><span class="xml">          0.568417  ,  0.5645389 ],</span></span><br><span class="line"><span class="xml">        [ 0.10261729, -0.6108351 , -0.13857493, ...,  1.0849315 ,</span></span><br><span class="line"><span class="xml">         -0.04182164,  1.833772  ],</span></span><br><span class="line"><span class="xml">        [ 0.70634294, -0.5957784 , -0.5884442 , ..., -1.1991992 ,</span></span><br><span class="line"><span class="xml">          0.18217663, -0.7455608 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.21443701,  2.2860959 ,  1.3482761 , ...,  1.3715926 ,</span></span><br><span class="line"><span class="xml">         -0.908046  , -1.4794428 ],</span></span><br><span class="line"><span class="xml">        [ 2.3567638 ,  1.103051  ,  0.35473076, ..., -0.83630645,</span></span><br><span class="line"><span class="xml">         -0.15155283, -0.19624123],</span></span><br><span class="line"><span class="xml">        [-0.15094325, -0.6325083 ,  1.9859867 , ...,  0.93023133,</span></span><br><span class="line"><span class="xml">          1.4148763 ,  1.5216227 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[-0.666422  ,  0.16517723, -0.24814457, ..., -0.38779825,</span></span><br><span class="line"><span class="xml">         -0.55923134,  0.28171122],</span></span><br><span class="line"><span class="xml">        [ 0.50092065,  1.0917888 , -2.573731  , ..., -1.5025382 ,</span></span><br><span class="line"><span class="xml">          0.15940215,  1.083392  ],</span></span><br><span class="line"><span class="xml">        [-2.4639432 , -2.5800378 , -0.00995448, ..., -1.0277953 ,</span></span><br><span class="line"><span class="xml">          0.65327126,  0.6751699 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [-0.38669026, -0.8761874 ,  1.9369066 , ..., -0.4574192 ,</span></span><br><span class="line"><span class="xml">          0.87384427, -2.539474  ],</span></span><br><span class="line"><span class="xml">        [-1.1602821 ,  0.8352534 , -0.03141081, ...,  1.163112  ,</span></span><br><span class="line"><span class="xml">          0.3594764 ,  0.49750358],</span></span><br><span class="line"><span class="xml">        [ 0.60312474,  0.31406295,  0.06573463, ...,  1.126251  ,</span></span><br><span class="line"><span class="xml">         -1.7025791 ,  0.2925642 ]]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>

<p>为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以<strong>使用⋯符号表示取多个维度上所有的数据</strong>，其中维度的数量需根据规则自动推断：当切片方式出现⋯符号时，⋯符号左边的维度将自动对齐到最左边，⋯符号右边的维度将自动对齐到最右边，此时系统再自动推断⋯符号代表的维度数量<br></p>
<ul>
<li>a,⋯,b<br>a 维度对齐到最左边，b 维度对齐到最右边，中间的维度全部读取，其他维度按a/b 的方式读取</li>
<li>a,⋯</li>
<li>⋯,b</li>
<li>⋯</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[<span class="number">0</span>:<span class="number">2</span>,...,<span class="number">1</span>:] <span class="comment"># 高宽维度全部采集</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">395,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">32</span>, <span class="attr">32</span>, <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[-0.55588394,  1.3454418 ],</span></span><br><span class="line"><span class="xml">         [-0.49143514,  0.22605067],</span></span><br><span class="line"><span class="xml">         [ 0.91525185,  0.29261518],</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x[<span class="number">2</span>:,...] <span class="comment"># 高、宽、通道维度全部采集，等价于x[2:]</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">399,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">32</span>, <span class="attr">32</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[-2.96695441e-01, -1.78035438e+00, -2.28262997e+00],</span></span><br></pre></td></tr></tbody></table></figure></div>

<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>在神经网络运算过程中，<strong>维度变换是最核心的张量操作</strong>，通过维度变换可以将数据任意地切换形式。</p>
<ul>
<li>达到了给每个输入样本的输出节点共享偏置向量的逻辑。<br><br>为了实现这种运算方式，我们将偏置向量𝒃插入一个新的维度，并把它定义为Batch 维度，然后在Batch 维度将数据复制1 份，得到变换后的𝐁′，新的shape 为[2,3]。这一系列的操作就是维度变换操作。</li>
<li>基本的维度变换操作函数包含了<strong>改变视图reshape、插入新维度expand_dims，删除维度squeeze、交换维度transpose、复制数据tile</strong>等函数。</li>
</ul>
<h3 id="（1）视图"><a href="#（1）视图" class="headerlink" title="（1）视图"></a>（1）视图</h3><ul>
<li><p>张量的存储体现在张量在内存上保存为一段<strong>连续的内存区域</strong><br><br>内存并不支持这个维度层级概念，只能<strong>以平铺方式按序写入内存</strong>，因此这种层级关系需要人为管理，也就是说，每个张量的存储顺序需要人为跟踪。为了方便表达，我们把<strong>张量shape 列表中相对靠左侧的维度叫作大维度</strong>，shape 列表中相对靠右侧的维度叫作小维度。<br><br>比如[2,4,4,3]的张量中，图片数量维度与通道数量相比，图片数量叫作大维度，通道数叫作小维度。在优先写入小维度的设定下，上述张量的内存布局为1,2,3…95</p>
</li>
<li><p>同一个存储，从不同的角度观察数据，可以产生不同的视图，这就是存储与视图的关系。</p>
</li>
<li><p>通过<strong>tf.reshape 视图改变函数</strong>产生不同的视图</p>
</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.range(<span class="number">96</span>)</span><br><span class="line">x=tf.reshape(x,[<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>]) <span class="comment"># 改变x 的视图，获得4D 张量，存储并未改变</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>由于存储时数据只有平坦结构，与数据的逻辑结构是分离的，因此如果新的逻辑结构不需要改变数据的存储方式，就可以节省大量计算资源，这也是改变视图操作的优势。</p>
<hr>
<ul>
<li>从语法上来说，视图变换只需要满足新视图的元素总量与存储区域大小相等即可。</li>
<li>改变视图是神经网络中非常常见的操作，可以通过串联多个reshape 操作来实现复杂逻辑。</li>
<li>在 TensorFlow 中，可以通过张量的ndim 和shape 成员属性获得张量的维度数和形状。</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x.ndim,x.shape <span class="comment"># 获取张量的维度数和形状列表</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">(4, TensorShape([2, 4, 4, 3]))</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.reshape(x,[<span class="number">2</span>,<span class="number">-1</span>])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">407,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">48</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,</span></span><br><span class="line"><span class="xml">        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,</span></span><br><span class="line"><span class="xml">        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],</span></span><br><span class="line"><span class="xml">       [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,</span></span><br><span class="line"><span class="xml">        64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,</span></span><br><span class="line"><span class="xml">        80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<p>其中，参数−1表示当前轴上长度需要根据张量总元素不变的法则自动推导，上面的−1可以推导为48。 [2,48]</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.reshape(x,[<span class="number">2</span>,<span class="number">-1</span>,<span class="number">3</span>]) <span class="comment">#[2,16,3]</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">409,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">16</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[ 0,  1,  2],</span></span><br><span class="line"><span class="xml">        [ 3,  4,  5],</span></span><br><span class="line"><span class="xml">        [ 6,  7,  8],</span></span><br><span class="line"><span class="xml">        [ 9, 10, 11],</span></span><br><span class="line"><span class="xml">        [12, 13, 14],</span></span><br><span class="line"><span class="xml">        [15, 16, 17],</span></span><br><span class="line"><span class="xml">        [18, 19, 20],</span></span><br><span class="line"><span class="xml">        [21, 22, 23],</span></span><br><span class="line"><span class="xml">        [24, 25, 26],</span></span><br><span class="line"><span class="xml">        [27, 28, 29],</span></span><br><span class="line"><span class="xml">        [30, 31, 32],</span></span><br><span class="line"><span class="xml">        [33, 34, 35],</span></span><br><span class="line"><span class="xml">        [36, 37, 38],</span></span><br><span class="line"><span class="xml">        [39, 40, 41],</span></span><br><span class="line"><span class="xml">        [42, 43, 44],</span></span><br><span class="line"><span class="xml">        [45, 46, 47]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[48, 49, 50],</span></span><br><span class="line"><span class="xml">        [51, 52, 53],</span></span><br><span class="line"><span class="xml">        [54, 55, 56],</span></span><br><span class="line"><span class="xml">        [57, 58, 59],</span></span><br><span class="line"><span class="xml">        [60, 61, 62],</span></span><br><span class="line"><span class="xml">        [63, 64, 65],</span></span><br><span class="line"><span class="xml">        [66, 67, 68],</span></span><br><span class="line"><span class="xml">        [69, 70, 71],</span></span><br><span class="line"><span class="xml">        [72, 73, 74],</span></span><br><span class="line"><span class="xml">        [75, 76, 77],</span></span><br><span class="line"><span class="xml">        [78, 79, 80],</span></span><br><span class="line"><span class="xml">        [81, 82, 83],</span></span><br><span class="line"><span class="xml">        [84, 85, 86],</span></span><br><span class="line"><span class="xml">        [87, 88, 89],</span></span><br><span class="line"><span class="xml">        [90, 91, 92],</span></span><br><span class="line"><span class="xml">        [93, 94, 95]]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<h3 id="（2）增删维度"><a href="#（2）增删维度" class="headerlink" title="（2）增删维度"></a>（2）增删维度</h3><ul>
<li>tf.expand_dims 的axis 为正时，表示在当前维度之前插入一个新维度；为负时，表示当前维度之后插入一个新的维度</li>
<li>删除维度只能删除长度为1的维度，也不会改变张量的存储。tf.squeeze(x, axis)函数，axis 参数为待删除的维度的索引号，</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">28</span>,<span class="number">28</span>],maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">429,</span> <span class="attr">shape</span>=<span class="string">(28,</span> <span class="attr">28</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[7, 8, 0, 3, 7, 2, 9, 0, 1, 9, 5, 8, 1, 4, 8, 5, 8, 1, 7, 9, 7, 4,</span></span><br><span class="line"><span class="xml">        4, 0, 9, 3, 2, 4],</span></span><br><span class="line"><span class="xml">       [7, 5, 7, 9, 0, 6, 3, 7, 2, 6, 2, 2, 6, 2, 2, 5, 9, 2, 8, 2, 7, 8,</span></span><br><span class="line"><span class="xml">        8, 8, 1, 2, 5, 3],</span></span><br><span class="line"><span class="xml">       [2, 3, 2, 6, 4, 9, 8, 2, 0, 3, 3, 8, 7, 5, 1, 9, 1, 3, 6, 4, 5, 7,</span></span><br><span class="line"><span class="xml">        3, 9, 9, 3, 8, 7],</span></span><br><span class="line"><span class="xml">       [0, 1, 9, 0, 9, 5, 6, 3, 4, 3, 0, 4, 8, 5, 7, 4, 0, 5, 0, 1, 6, 2,</span></span><br><span class="line"><span class="xml">        2, 7, 8, 0, 2, 4],</span></span><br><span class="line"><span class="xml">       [9, 7, 8, 5, 3, 5, 4, 6, 1, 3, 7, 7, 7, 4, 9, 3, 2, 4, 2, 3, 3, 4,</span></span><br><span class="line"><span class="xml">        1, 3, 6, 1, 9, 9],</span></span><br><span class="line"><span class="xml">       [4, 3, 8, 7, 9, 0, 4, 9, 3, 0, 0, 4, 3, 5, 2, 7, 1, 9, 4, 2, 6, 7,</span></span><br><span class="line"><span class="xml">        6, 6, 8, 9, 5, 6],</span></span><br><span class="line"><span class="xml">       [3, 4, 9, 0, 5, 4, 5, 5, 8, 5, 1, 1, 8, 2, 6, 6, 6, 9, 2, 7, 0, 1,</span></span><br><span class="line"><span class="xml">        4, 3, 0, 5, 0, 3],</span></span><br><span class="line"><span class="xml">       [6, 2, 2, 1, 1, 4, 0, 6, 3, 4, 4, 2, 4, 6, 0, 2, 8, 8, 4, 0, 7, 9,</span></span><br><span class="line"><span class="xml">        1, 8, 6, 4, 1, 1],</span></span><br><span class="line"><span class="xml">       [8, 4, 8, 4, 6, 5, 5, 4, 7, 4, 1, 9, 6, 2, 8, 9, 9, 1, 6, 1, 8, 9,</span></span><br><span class="line"><span class="xml">        0, 5, 6, 0, 1, 5],</span></span><br><span class="line"><span class="xml">       [9, 2, 4, 4, 4, 2, 7, 6, 1, 6, 4, 0, 1, 6, 1, 9, 9, 0, 9, 2, 2, 8,</span></span><br><span class="line"><span class="xml">        8, 8, 1, 2, 8, 4],</span></span><br><span class="line"><span class="xml">       [6, 6, 1, 5, 6, 0, 6, 3, 3, 4, 1, 6, 7, 8, 4, 4, 4, 0, 7, 0, 7, 2,</span></span><br><span class="line"><span class="xml">        0, 6, 4, 7, 4, 5],</span></span><br><span class="line"><span class="xml">       [2, 3, 8, 4, 6, 8, 7, 6, 8, 1, 5, 1, 8, 1, 6, 4, 1, 2, 6, 4, 4, 7,</span></span><br><span class="line"><span class="xml">        3, 7, 2, 3, 0, 5],</span></span><br><span class="line"><span class="xml">       [7, 1, 0, 2, 1, 5, 9, 6, 1, 8, 7, 5, 4, 2, 6, 1, 4, 2, 4, 3, 2, 6,</span></span><br><span class="line"><span class="xml">        3, 6, 5, 5, 3, 9],</span></span><br><span class="line"><span class="xml">       [6, 5, 7, 2, 7, 8, 2, 1, 9, 4, 2, 0, 7, 4, 5, 8, 4, 9, 7, 0, 6, 0,</span></span><br><span class="line"><span class="xml">        6, 9, 9, 7, 7, 0],</span></span><br><span class="line"><span class="xml">       [0, 2, 4, 3, 1, 3, 7, 5, 7, 7, 1, 3, 6, 2, 5, 5, 4, 7, 9, 8, 0, 8,</span></span><br><span class="line"><span class="xml">        0, 3, 6, 4, 9, 2],</span></span><br><span class="line"><span class="xml">       [2, 8, 8, 4, 5, 7, 3, 2, 1, 7, 1, 6, 5, 7, 4, 3, 3, 6, 6, 8, 9, 8,</span></span><br><span class="line"><span class="xml">        3, 0, 0, 9, 4, 7],</span></span><br><span class="line"><span class="xml">       [1, 9, 8, 2, 2, 9, 2, 9, 2, 1, 0, 4, 7, 6, 1, 4, 1, 8, 9, 4, 8, 5,</span></span><br><span class="line"><span class="xml">        5, 5, 2, 0, 0, 9],</span></span><br><span class="line"><span class="xml">       [4, 3, 9, 2, 1, 5, 0, 2, 9, 6, 5, 9, 9, 0, 1, 8, 2, 8, 6, 1, 5, 8,</span></span><br><span class="line"><span class="xml">        7, 7, 8, 3, 8, 0],</span></span><br><span class="line"><span class="xml">       [7, 4, 8, 6, 9, 7, 6, 3, 3, 7, 8, 5, 2, 5, 9, 4, 9, 4, 0, 2, 5, 3,</span></span><br><span class="line"><span class="xml">        1, 0, 5, 6, 4, 5],</span></span><br><span class="line"><span class="xml">       [8, 2, 5, 8, 6, 9, 4, 6, 8, 7, 3, 9, 7, 2, 1, 7, 1, 0, 8, 1, 1, 6,</span></span><br><span class="line"><span class="xml">        3, 7, 7, 9, 8, 4],</span></span><br><span class="line"><span class="xml">       [2, 7, 3, 6, 1, 5, 7, 1, 8, 0, 2, 5, 0, 1, 3, 8, 5, 3, 6, 8, 1, 4,</span></span><br><span class="line"><span class="xml">        1, 2, 1, 5, 5, 3],</span></span><br><span class="line"><span class="xml">       [8, 8, 8, 6, 1, 3, 0, 7, 7, 7, 5, 6, 2, 3, 6, 9, 6, 3, 7, 5, 6, 5,</span></span><br><span class="line"><span class="xml">        7, 1, 7, 1, 7, 3],</span></span><br><span class="line"><span class="xml">       [3, 7, 9, 6, 5, 9, 3, 2, 3, 6, 6, 6, 1, 3, 3, 2, 5, 5, 4, 6, 2, 8,</span></span><br><span class="line"><span class="xml">        5, 0, 7, 2, 1, 7],</span></span><br><span class="line"><span class="xml">       [4, 6, 3, 1, 8, 5, 9, 1, 3, 5, 0, 8, 1, 9, 6, 2, 1, 4, 5, 1, 2, 4,</span></span><br><span class="line"><span class="xml">        2, 9, 7, 6, 3, 1],</span></span><br><span class="line"><span class="xml">       [0, 1, 1, 6, 9, 7, 1, 7, 8, 4, 4, 4, 3, 2, 0, 1, 9, 4, 1, 5, 8, 7,</span></span><br><span class="line"><span class="xml">        6, 2, 5, 8, 8, 3],</span></span><br><span class="line"><span class="xml">       [0, 2, 4, 2, 3, 9, 5, 8, 6, 4, 8, 3, 6, 4, 5, 9, 3, 4, 8, 8, 3, 2,</span></span><br><span class="line"><span class="xml">        9, 1, 7, 8, 2, 9],</span></span><br><span class="line"><span class="xml">       [8, 5, 2, 5, 2, 4, 9, 3, 4, 8, 4, 0, 5, 9, 1, 6, 4, 9, 8, 9, 4, 0,</span></span><br><span class="line"><span class="xml">        4, 6, 8, 2, 3, 4],</span></span><br><span class="line"><span class="xml">       [3, 8, 7, 5, 0, 6, 7, 8, 0, 8, 2, 8, 9, 9, 1, 6, 6, 7, 3, 2, 0, 6,</span></span><br><span class="line"><span class="xml">        1, 1, 8, 4, 6, 1]])></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.expand_dims(x,axis=<span class="number">2</span>) <span class="comment"># axis=2 表示宽维度后面的一个维度</span></span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">431,</span> <span class="attr">shape</span>=<span class="string">(28,</span> <span class="attr">28</span>, <span class="attr">1</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br></pre></td></tr></tbody></table></figure></div>

<p>插入一个新维度后，数据的存储顺序并没有改变，仅仅是在插入一个新的维度后，改变了数据的视图。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.expand_dims(x,axis=<span class="number">0</span>) <span class="comment"># 高维度之前插入新维度</span></span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">433,</span> <span class="attr">shape</span>=<span class="string">(1,</span> <span class="attr">28</span>, <span class="attr">28</span>, <span class="attr">1</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.squeeze(x, axis=<span class="number">0</span>) <span class="comment"># 删除图片数量维度</span></span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">434,</span> <span class="attr">shape</span>=<span class="string">(28,</span> <span class="attr">28</span>, <span class="attr">1</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.squeeze(x, axis=<span class="number">2</span>) <span class="comment"># 删除图片通道数维度</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>如果不指定维度参数axis，即tf.squeeze(x)，那么它会默认删除所有长度为1 的维度</p>
<h3 id="（3）交换维度"><a href="#（3）交换维度" class="headerlink" title="（3）交换维度"></a>（3）交换维度</h3><ul>
<li>tf.transpose(x, perm) ，perm表示新维度的顺序List</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line">tf.transpose(x,perm=[<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">443,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">3</span>, <span class="attr">32</span>, <span class="attr">32</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[ 1.16737115e+00,  6.64409459e-01,  1.90105021e+00, ...,</span></span><br><span class="line"><span class="xml">          -9.63927925e-01,  5.60056984e-01,  1.88317168e+00],</span></span><br><span class="line"><span class="xml">         [ 2.51950324e-01, -4.93862867e-01, -5.70928037e-01, ...,</span></span><br><span class="line"><span class="xml">          -7.66403139e-01, -6.34723306e-01,  3.61261755e-01],</span></span><br><span class="line"><span class="xml">         [ 4.89662796e-01, -2.39876032e+00, -1.41122067e+00, ...,</span></span><br><span class="line"><span class="xml">          -1.10434747e+00,  2.82260865e-01, -2.60873348e-01],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-9.89986837e-01,  2.53070481e-02, -1.87445736e+00, ...,</span></span><br><span class="line"><span class="xml">          -3.18429410e-01, -3.64445066e+00, -5.95382750e-01],</span></span><br><span class="line"><span class="xml">         [-1.98994026e-01,  2.81752869e-02, -1.91701210e+00, ...,</span></span><br><span class="line"><span class="xml">           6.10095263e-01, -5.12470722e-01,  5.13144195e-01],</span></span><br><span class="line"><span class="xml">         [-1.62783742e+00, -1.63309872e+00,  1.22629754e-01, ...,</span></span><br><span class="line"><span class="xml">           8.35806653e-02, -1.10099681e-01, -8.15727472e-01]],</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>需要注意的是，通过tf.transpose 完成维度交换后，张量的存储顺序已经改变，视图也随之改变，后续的所有操作必须基于新的存续顺序和视图进行。</p>
<h3 id="（3）复制数据"><a href="#（3）复制数据" class="headerlink" title="（3）复制数据"></a>（3）复制数据</h3><p>考虑𝒀 = 𝑿@𝑾 + 𝒃的例子，偏置𝒃插入样本数的新维度后，需要在新维度上复制Batch Size 份数据，将shape 变为与𝑿@𝑾一致后，才能完成张量相加运算。</p>
<ul>
<li><strong>tf.tile(x, multiples)</strong>，multiples 分别指定了每个维度上面的复制倍数，对应位置为1 表明不复制，为2 表明新长度为原来长度的2 倍，即数据复制一份，以此类推。</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">b = tf.constant([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">452,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([1,</span> <span class="attr">2</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">b = tf.expand_dims(b, axis=<span class="number">0</span>) <span class="comment"># 插入新维度，变成矩阵</span></span><br><span class="line">b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">454,</span> <span class="attr">shape</span>=<span class="string">(1,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([[1,</span> <span class="attr">2</span>]])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">b = tf.tile(b, multiples=[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">456,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[1, 2],</span></span><br><span class="line"><span class="xml">       [1, 2]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<p><strong>实际上，上述插入维度和复制数据的步骤并不需要我们手动执行，TensorFlow 会自动完成，这就是自动扩展功能。</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.range(<span class="number">4</span>)</span><br><span class="line">x = tf.reshape(x, [<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">x = tf.tile(x,multiples=[<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 列维度复制一份</span></span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">464,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">4</span>), <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[0, 1, 0, 1],</span></span><br><span class="line"><span class="xml">       [2, 3, 2, 3]])></span></span><br></pre></td></tr></tbody></table></figure></div>

<p><strong>tf.tile 会创建一个新的张量来保存复制后的张量，由于复制操作涉及大<br>量数据的读写IO 运算，计算代价相对较高。</strong></p>
<h2 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h2><ul>
<li>广播机制（自动扩展机制），轻量级张量复制手段。</li>
<li>在逻辑上扩展张量数据的形状，但是只会在需要时才会执行实际存储复制操作</li>
<li>tf.tile 会创建一个新的张量，执行复制IO 操作，并保存复制后的张量数据，而Broadcasting 并不会立即复制数据，它会在逻辑上改变张量的形状，使得视图上变成了复制后的形状</li>
<li>对于用户来说，Broadcasting 和tf.tile 复制的最终效果是一样的，操作对用户透明，但是Broadcasting 机制节省了大量计算资源，建议在运算过程中尽可能地利用Broadcasting 机制提高计算效率。</li>
</ul>
<p>自动调用Broadcasting函数tf.broadcast_to(x, new_shape)，将两者shape 扩张为相同的[2,3]</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">w = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">3</span>])</span><br><span class="line"><span class="comment">#y = x@w + b # 手动扩展，并相加</span></span><br><span class="line">y = x@w + tf.broadcast_to(b,[<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 手动扩展，并相加 与上式等效</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">A = tf.random.normal([<span class="number">32</span>,<span class="number">1</span>])</span><br><span class="line">tf.broadcast_to(A, [<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">494,</span> <span class="attr">shape</span>=<span class="string">(2,</span> <span class="attr">32</span>, <span class="attr">32</span>, <span class="attr">3</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        [[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        [[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        [[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        [[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">        [[ 0.76788235,  0.76788235,  0.76788235],</span></span><br><span class="line"><span class="xml">         [ 0.2562372 ,  0.2562372 ,  0.2562372 ],</span></span><br><span class="line"><span class="xml">         [-1.0664971 , -1.0664971 , -1.0664971 ],</span></span><br><span class="line"><span class="xml">         ...,</span></span><br><span class="line"><span class="xml">         [-0.5424432 , -0.5424432 , -0.5424432 ],</span></span><br><span class="line"><span class="xml">         [ 1.2622304 ,  1.2622304 ,  1.2622304 ],</span></span><br><span class="line"><span class="xml">         [ 0.92822355,  0.92822355,  0.92822355]]],</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>在𝑐维度上，张量已经有2 个特征数据，新shape 对应维度的长度为𝑐(𝑐 ≠ 2，如𝑐=3)，那么当前维度上的这2 个特征无法普适到其它位置，故<strong>不满足普适性原则，无法应用Broadcasting 机制，将会触发错误</strong></p>
<h2 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h2><ul>
<li>tf.add, tf.subtract, tf.multiply, tf.divide / 直接运算符</li>
<li>整除和余除也是常见的运算之一，分别通过//和%运算符实现</li>
<li>乘方：tf.pow(x, a) 或者 **</li>
<li>指数：tf.pow(x, a) 或者 **</li>
<li>平方根：tf.pow(x,1/a)</li>
<li>tf.square(x)和tf.sqrt(x)</li>
<li>tf.exp(x) 自然指数</li>
<li>自然对数可以通过tf.math.log(x), 如果希望计算其它底数的对数，可以根据对数的换底公式间接实现</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.range(<span class="number">5</span>)</span><br><span class="line">b = tf.constant(<span class="number">2</span>)</span><br><span class="line">a//b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">500,</span> <span class="attr">shape</span>=<span class="string">(5,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([0,</span> <span class="attr">0</span>, <span class="attr">1</span>, <span class="attr">1</span>, <span class="attr">2</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a%b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">501,</span> <span class="attr">shape</span>=<span class="string">(5,),</span> <span class="attr">dtype</span>=<span class="string">int32,</span> <span class="attr">numpy</span>=<span class="string">array([0,</span> <span class="attr">1</span>, <span class="attr">0</span>, <span class="attr">1</span>, <span class="attr">0</span>])></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x=tf.constant([<span class="number">1.</span>,<span class="number">4.</span>,<span class="number">9.</span>])</span><br><span class="line">x**(<span class="number">0.5</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">504,</span> <span class="attr">shape</span>=<span class="string">(3,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([1.,</span> <span class="attr">2.</span>, <span class="attr">3.</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.exp(<span class="number">1.</span>)</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">506,</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">2.7182817</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>])</span><br><span class="line">x = <span class="number">10</span>**x</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">509,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([10.</span>     , <span class="attr">99.99999</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.math.log(x)/tf.math.log(<span class="number">10.</span>) <span class="comment"># 换底公式</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">513,</span> <span class="attr">shape</span>=<span class="string">(2,),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">array([1.,</span> <span class="attr">2.</span>], <span class="attr">dtype</span>=<span class="string">float32)</span>></span></span></span><br></pre></td></tr></tbody></table></figure></div>

<p><strong>矩阵相乘</strong></p>
<ul>
<li>@可以实现</li>
<li>tf.matmul(a, b)</li>
<li>当张量𝑨和𝑩维度数大于2 时，TensorFlow 会选择𝑨和𝑩的最后两个维度进行矩阵相乘，前面所有的维度都视作Batch维度。</li>
<li>根据矩阵相乘的定义，𝑨和𝑩能够矩阵相乘的条件是，𝑨的倒数第一个维度长度(列)和𝑩的倒数第二个维度长度(行)必须相等。</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">2</span>])</span><br><span class="line">a@b <span class="comment"># 批量形式的矩阵相乘</span></span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">526,</span> <span class="attr">shape</span>=<span class="string">(4,</span> <span class="attr">3</span>, <span class="attr">28</span>, <span class="attr">2</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[[-1.23890102e-01,  3.60697842e+00],</span></span><br><span class="line"><span class="xml">         [ 8.38998032e+00,  8.17345238e+00],</span></span><br><span class="line"><span class="xml">         [-8.66496563e+00, -1.16394572e-01],</span></span><br></pre></td></tr></tbody></table></figure></div>

<p>矩阵相乘函数同样支持自动Broadcasting 机制</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">32</span>,<span class="number">16</span>])</span><br><span class="line">a@b</span><br></pre></td></tr></tbody></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">markdown</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag"><<span class="name">tf.Tensor:</span> <span class="attr">id</span>=<span class="string">552,</span> <span class="attr">shape</span>=<span class="string">(4,</span> <span class="attr">28</span>, <span class="attr">16</span>), <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=</span></span></span><br><span class="line"><span class="xml">array([[[  4.6358614 ,   2.3656723 ,   3.614833  , ...,  -2.641783  ,</span></span><br><span class="line"><span class="xml">           7.738894  ,  10.538157  ],</span></span><br><span class="line"><span class="xml">        [  2.527424  ,   8.58602   ,  -3.9890842 , ...,   1.2290429 ,</span></span><br><span class="line"><span class="xml">          -3.661516  ,  -5.3423166 ],</span></span><br><span class="line"><span class="xml">        [  4.217289  ,  -1.6918703 ,  13.683196  , ...,  -0.7053379 ,</span></span><br><span class="line"><span class="xml">           6.743267  ,  -7.642796  ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [  2.971324  ,   1.1276051 ,   4.888926  , ...,   1.18856   ,</span></span><br><span class="line"><span class="xml">          -0.6660559 ,  -1.3058037 ],</span></span><br><span class="line"><span class="xml">        [  1.9013156 ,  -5.820588  ,   2.6424277 , ...,   8.058013  ,</span></span><br><span class="line"><span class="xml">          10.838149  ,  -0.4928334 ],</span></span><br><span class="line"><span class="xml">        [  2.5643663 ,   9.057759  ,   0.48165786, ...,   4.6663036 ,</span></span><br><span class="line"><span class="xml">          -0.7325015 ,  -0.7360229 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ -0.2655417 ,   5.77299   ,  -8.423078  , ...,  -4.6590757 ,</span></span><br><span class="line"><span class="xml">          -3.7937422 ,  -1.0000533 ],</span></span><br><span class="line"><span class="xml">        [ -1.7784729 ,  -3.5993726 ,   0.04143755, ...,  12.379929  ,</span></span><br><span class="line"><span class="xml">           2.1767313 ,  -4.5347605 ],</span></span><br><span class="line"><span class="xml">        [ -8.358639  ,   2.5583687 , -14.023441  , ...,   2.295009  ,</span></span><br><span class="line"><span class="xml">          -5.0464425 ,  -2.4835758 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [  4.7978597 ,  -1.3027284 ,  11.156745  , ...,  -4.826609  ,</span></span><br><span class="line"><span class="xml">           1.9011586 ,  -4.150916  ],</span></span><br><span class="line"><span class="xml">        [ -1.4297161 ,  -1.003312  ,   7.547075  , ...,   0.6288889 ,</span></span><br><span class="line"><span class="xml">          -3.4253068 ,  -0.32016313],</span></span><br><span class="line"><span class="xml">        [  0.88476694,   6.1622086 ,   4.383214  , ...,  -0.8960331 ,</span></span><br><span class="line"><span class="xml">          -5.774914  ,  -3.7017817 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ -4.300797  ,  -8.4093685 ,  -3.1488764 , ...,   0.68959624,</span></span><br><span class="line"><span class="xml">          -1.2566758 ,  -6.290199  ],</span></span><br><span class="line"><span class="xml">        [ -3.4279275 ,   1.8672839 ,   4.0368567 , ...,  -0.392429  ,</span></span><br><span class="line"><span class="xml">          -9.380967  ,   5.9825706 ],</span></span><br><span class="line"><span class="xml">        [  0.6960955 ,  -1.994213  ,  -5.1472654 , ...,  -0.51111895,</span></span><br><span class="line"><span class="xml">          -5.1857624 ,   1.654432  ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [ -2.644772  ,  -2.2885127 ,  -1.1539077 , ...,   0.5785912 ,</span></span><br><span class="line"><span class="xml">           6.2637873 ,  -4.5564237 ],</span></span><br><span class="line"><span class="xml">        [  5.1499605 ,   4.0891433 ,   1.9962361 , ...,   4.180484  ,</span></span><br><span class="line"><span class="xml">           3.9339638 ,  -3.6846066 ],</span></span><br><span class="line"><span class="xml">        [ -4.6220136 ,   4.1199164 , -12.855832  , ...,  -5.286996  ,</span></span><br><span class="line"><span class="xml">          -4.2426867 ,   7.1902914 ]],</span></span><br><span class="line"></span><br><span class="line"><span class="xml">       [[ -0.8115117 ,  -3.4815786 ,  -1.2164723 , ...,   4.7878146 ,</span></span><br><span class="line"><span class="xml">          -9.417316  ,   6.9470363 ],</span></span><br><span class="line"><span class="xml">        [ -4.9162726 ,   0.88088876,  -6.619688  , ...,  -3.0081198 ,</span></span><br><span class="line"><span class="xml">          -1.3455021 ,   2.4650884 ],</span></span><br><span class="line"><span class="xml">        [ -2.0685956 ,  -9.58242   ,   7.0207357 , ...,  -7.0429354 ,</span></span><br><span class="line"><span class="xml">           3.7462664 ,  -2.1684577 ],</span></span><br><span class="line"><span class="xml">        ...,</span></span><br><span class="line"><span class="xml">        [  2.9409153 ,  -1.4385134 ,  -0.9850677 , ...,   0.37230483,</span></span><br><span class="line"><span class="xml">           9.6056595 ,   3.9546096 ],</span></span><br><span class="line"><span class="xml">        [  3.544051  ,  10.121863  ,  -1.4211951 , ...,   7.4306874 ,</span></span><br><span class="line"><span class="xml">          -9.355392  ,   2.2145257 ],</span></span><br><span class="line"><span class="xml">        [ -1.5322605 ,  -2.475702  ,  -0.6080578 , ...,  -8.667455  ,</span></span><br><span class="line"><span class="xml">           1.0644337 ,   3.5168955 ]]], dtype=float32)></span></span><br></pre></td></tr></tbody></table></figure></div>

<p>上述运算自动将变量b 扩展为公共shape：[4,32,16]</p>
</body></html></div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning    </a><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-full"><a href="/2020/02/27/2020-02-27-unit5/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Tensorflow进阶</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/02/27/2020-02-27-unit5/" title="Tensorflow进阶"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-27</div><div class="relatedPosts_title">Tensorflow进阶</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/03/2020-03-03-unit6/" title="神经网络"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-03</div><div class="relatedPosts_title">神经网络</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Iyin</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script id="canvas_nest" color="189,207,69" opacity="0.9" zIndex="-1" count="70" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>