<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>2.1 Attention | Iyin's blog</title><meta name="description" content="基于transformers的自然语言处理(NLP)入门"><meta name="keywords" content="自然语言处理,transformers"><meta name="author" content="Iyin,yinwein@foxmail.com"><meta name="copyright" content="Iyin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/plant.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="2.1 Attention"><meta name="twitter:description" content="基于transformers的自然语言处理(NLP)入门"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="2.1 Attention"><meta property="og:url" content="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.1/"><meta property="og:site_name" content="Iyin's blog"><meta property="og:description" content="基于transformers的自然语言处理(NLP)入门"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.1/"><link rel="prev" title="2.2 Transformer" href="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.2/"><link rel="next" title="（一）Transformers在NLP中的兴起" href="http://yoursite.com/2021/08/15/2021-08-15-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A81/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web:600&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Iyin's blog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://ae01.alicdn.com/kf/Hb280d3c31450463cb3dc638bf33ca871f.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">13</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#seq2seq模型"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">seq2seq模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#细节"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">细节</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#RNN"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">RNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#词向量"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">词向量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#编码与解码"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">编码与解码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Attention"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#码"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">码</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#seq2seq模型"><span class="toc-number">1.</span> <span class="toc-text">seq2seq模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#细节"><span class="toc-number">2.</span> <span class="toc-text">细节</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN"><span class="toc-number">2.1.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#词向量"><span class="toc-number">2.2.</span> <span class="toc-text">词向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#编码与解码"><span class="toc-number">2.3.</span> <span class="toc-text">编码与解码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Attention"><span class="toc-number">3.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#码"><span class="toc-number">4.</span> <span class="toc-text">码</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://s2.ax1x.com/2020/02/27/3aTBOx.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">2.1 Attention</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2021-08-18<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2021-08-18</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><p>搬运自<a href="https://github.com/datawhalechina/learn-nlp-with-transformers.git" target="_blank" rel="noopener">https://github.com/datawhalechina/learn-nlp-with-transformers.git</a></p>
<h1 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h1><p>NLP常用于生成任务的seq2seq结构。seq2seq模型结构在很多任务上都取得了成功，如：机器翻译、文本摘要、图像描述生成。</p>
<p>seq2seq模型是由编码器（Encoder）和解码器（Decoder）组成的。其中，编码器会处理输入序列中的每个元素，把这些信息<strong>转换为一个向量（称为上下文（context））</strong>。当我们处理完整个输入序列后，编码器把<strong>上下文（context）</strong>发送给解码器，解码器开始<strong>逐项生成输出序列中的元素。</strong></p>
<p><img alt="encoder-decode" data-src="/images/1-3-encoder-decoder.gif" src="/img/loading.gif" class="lazyload">动态图：encoder-decoder</p>
<h1 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>在机器翻译任务中，上下文（context）是一个向量（基本上是一个数字数组)。<strong>编码器和解码器在Transformer出现之前一般采用的是循环神经网络。</strong>关于循环神经网络，建议阅读 <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank" rel="noopener">Luis Serrano写的一篇关于循环神经网络</a>的精彩介绍。</p>
<p>可以在编写seq2seq模型的时候设置上下文向量的长度。<strong>这个长度是基于编码器 RNN 的隐藏层神经元的数量</strong>。在实际应用中，上下文向量的长度可能是 256，512 或者 1024。</p>
<p>根据设计，RNN 在每个时间步接受 2 个输入：</p>
<blockquote>
<p>什么叫时间步？</p>
</blockquote>
<ul>
<li>输入序列中的一个元素（在解码器的例子中，输入是指句子中的一个单词，最终被转化成一个向量）</li>
<li>一个  hidden state（隐藏层状态，也对应一个向量）</li>
</ul>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>如何把每个单词都转化为一个向量呢？我们使用一类称为 <strong>“word embedding” 的方法。这类方法把单词转换到一个向量空间，这种表示能够捕捉大量单词之间的语义信息</strong>（例如，king - man + woman = queen<a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" target="_blank" rel="noopener">例子来源</a>）。</p>
<p><img alt="我们在处理单词之前，需要把他们转换为向量。这个转换是使用 word embedding 算法来完成的。我们可以使用预训练好的 embeddings，或者在我们的数据集上训练自己的 embedding。通常 embedding 向量大小是 200 或者 300，为了简单起见，我们这里展示的向量长度是4" data-src="/images/1-5-word-vector.png" src="/img/loading.gif" class="lazyload"> </p>
<p>图：我们在处理单词之前，需要把他们转换为向量。这个转换是使用 word embedding 算法来完成的。我们可以使用预训练好的 embeddings，或者在我们的数据集上训练自己的 embedding。<strong>通常 embedding 向量大小是 200 或者 300</strong>，为了简单起见，我们这里展示的向量长度是4。上图左边每个单词对应中间一个4维的向量。</p>
<h2 id="编码与解码"><a href="#编码与解码" class="headerlink" title="编码与解码"></a>编码与解码</h2><p> RNN 模型：</p>
<p><img alt="rnn" data-src="/images/1-6-rnn.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>编码器和解码器在每个时间步处理输入，并得到输出。由于编码器和解码器都是 RNN，<strong>RNN 会根据当前时间步的输入，和前一个时间步的 hidden state（隐藏层状态），更新当前时间步的 hidden state（隐藏层状态）。</strong></p>
<p>让我们看下编码器的 hidden state（隐藏层状态）。注意，<strong>最后一个 hidden state（隐藏层状态）实际上是我们传给解码器的上下文（context）</strong>。<br><img alt data-src="/images/1-6-seq2seq.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>动态图：编码器相关</p>
<p>解码器也持有 hidden state（隐藏层状态），而且也需要把 hidden state（隐藏层状态）从一个时间步传递到下一个时间步。我们没有在上图中可视化解码器的 hidden state，是因为这个过程和解码器是类似的。</p>
<p>现在让我们用另一种方式来可视化序列到序列（seq2seq）模型。下面的动画会让我们更加容易理解模型。这种方法称为展开视图。其中，我们不只是显示一个解码器，而是在时间上展开，每个时间步都显示一个解码器。通过这种方式，我们可以看到每个时间步的输入和输出。</p>
<p><img alt data-src="/images/1-6-seq2seq-decoder.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>动态图：解码器相关</p>
<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>事实证明，上下文context向量是这类模型的瓶颈。这使得模型在处理长文本时面临非常大的挑战。</p>
<blockquote>
<p>文本长度*向量长度</p>
</blockquote>
<p>在 Bahdanau等2014发布的<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a> 和 Luong等2015年发布的<a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation
</a>两篇论文中，提出了一种解决方法。这 2 篇论文提出并改进了一种叫做注意力<strong>attetion</strong>的技术，它极大地提高了机器翻译的质量。注意力使得模型可以根据需要，关注到输入序列的相关部分。</p>
<p><img alt="在第7个时间步，注意力机制使得解码器在产生英语翻译之前，可以将注意力集中在 " student" 这个词（在法语里，是 "student" 的意思）。这种从输入序列放大相关信号的能力，使得注意力模型，比没有注意力的模型，产生更好的结果。" data-src="/images/1-7-attetion.png" src="/img/loading.gif" class="lazyload"> </p>
<p>图：在第 7 个时间步，注意力机制使得解码器在产生英语翻译之前，可以将注意力集中在 “student” 这个词（在法语里，是 “student” 的意思）。这种<strong>从输入序列放大相关信号</strong>的能力，使得注意力模型，比没有注意力的模型，产生更好的结果。</p>
<p>让我们继续从高层次来理解注意力模型。<strong>一个注意力模型不同于经典的序列到序列（seq2seq）模型，主要体现在 2 个方面：</strong></p>
<p>（一）首先，编码器会把更多的数据传递给解码器。<strong>编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态）:</strong></p>
<p><img alt data-src="/images/1-6-mt-1.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>动态图: 更多的信息传递给decoder</p>
<blockquote>
<p>隐藏层长度与 embedding 向量大小相同</p>
</blockquote>
<p>（二）第二，<strong>注意力模型的解码器在产生输出之前，做了一个额外的处理。</strong>为了把注意力集中在与该时间步相关的输入部分。解码器做了如下的处理：</p>
<ol>
<li>查看所有接收到的编码器的 hidden state（隐藏层状态）。其中，编码器中每个 hidden state（隐藏层状态）都对应到输入句子中一个单词。</li>
<li>给每个 hidden state（隐藏层状态）一个分数（我们先忽略这个分数的计算过程）。</li>
<li>将每个 hidden state（隐藏层状态）乘以经过 softmax 的对应的分数，从而，高分对应的  hidden state（隐藏层状态）会被放大，而低分对应的  hidden state（隐藏层状态）会被缩小。</li>
</ol>
<p><img alt data-src="/images/2021-08-18.png" src="/img/loading.gif" class="lazyload"></p>
<p>动态图：解码器attention</p>
<blockquote>
<p>如何分配注意力？</p>
</blockquote>
<p>这个加权平均的步骤是在解码器的每个时间步做的。<br>现在，让我们把所有内容都融合到下面的图中，来看看注意力模型的整个过程：</p>
<ol>
<li><p>注意力模型的解码器 RNN 的输入包括：<strong>一个embedding 向量，和一个初始化好的解码器 hidden state（隐藏层状态）。</strong></p>
<blockquote>
<p>encoder的隐层输出作为最一开始decoder的初始隐层</p>
<p>END作为输入，是句子标志位，</p>
</blockquote>
</li>
<li><p>RNN 处理上述的 2 个输入，产生一个输出和一个新的 hidden state（隐藏层状态 h4 向量），其中输出会被忽略。</p>
</li>
<li><p>注意力的步骤：我们使用编码器的 hidden state（隐藏层状态）和 h4 向量来计算这个时间步的上下文向量（C4）。</p>
</li>
<li><p>我们把 h4 和 C4 拼接起来，得到一个向量。</p>
</li>
<li><p>我们把这个向量输入一个前馈神经网络（这个网络是和整个模型一起训练的）。</p>
</li>
<li><p>前馈神经网络的输出的输出表示这个时间步输出的单词。</p>
</li>
<li><p>在下一个时间步重复这个步骤。<br><img alt data-src="/images/1-7-attention-pro.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>动态图：attention过程</p>
</li>
</ol>
<p>下图，我们使用另一种方式来可视化注意力，看看在每个解码的时间步中关注输入句子的哪些部分：</p>
<p><img alt data-src="/images/1-7-attention.gif" src="/img/loading.gif" class="lazyload"> </p>
<p>动态图：attention关注的词</p>
<blockquote>
<p>输入文本长度*向量长度，输出的文本长度与输入并不一定一致</p>
</blockquote>
<p>请注意，注意力模型不是无意识地把输出的第一个单词对应到输入的第一个单词。实际上，它从训练阶段学习到了如何在两种语言中对应单词的关系（在我们的例子中，是法语和英语）。</p>
<h1 id="码"><a href="#码" class="headerlink" title="码"></a>码</h1><p><a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank" rel="noopener">Luis Serrano写的一篇关于循环神经网络</a>的精彩介绍</p>
<p>学习注意力机制的代码实现，一定要看看基于 TensorFlow 的 神经机器翻译 (seq2seq) <a href="https://github.com/tensorflow/nmt" target="_blank" rel="noopener">指南</a>。</p>
</body></html></div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/transformers/">transformers    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.2/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>2.2 Transformer</span></div></a></div><div class="next-post pull_right"><a href="/2021/08/15/2021-08-15-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A81/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>（一）Transformers在NLP中的兴起</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/08/15/2021-08-15-基于transformers的自然语言处理(NLP)入门1/" title="（一）Transformers在NLP中的兴起"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-08-15</div><div class="relatedPosts_title">（一）Transformers在NLP中的兴起</div></div></a></div><div class="relatedPosts_item"><a href="/2021/08/18/2021-08-18-基于transformers的自然语言处理(NLP)入门2.2/" title="2.2 Transformer"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-08-18</div><div class="relatedPosts_title">2.2 Transformer</div></div></a></div><div class="relatedPosts_item"><a href="/2021/02/21/2021-02-21-NLP中文预训练模型泛化能力挑战赛（01_Docker提交）/" title="NLP中文预训练模型泛化能力挑战赛（01 Docker提交）"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-02-21</div><div class="relatedPosts_title">NLP中文预训练模型泛化能力挑战赛（01 Docker提交）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Iyin</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script id="canvas_nest" color="189,207,69" opacity="0.9" zIndex="-1" count="70" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>