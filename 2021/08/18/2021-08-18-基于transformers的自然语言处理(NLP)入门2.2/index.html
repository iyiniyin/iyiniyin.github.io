<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>2.2 Transformer | Iyin's blog</title><meta name="description" content="基于transformers的自然语言处理(NLP)入门"><meta name="keywords" content="自然语言处理,transformers"><meta name="author" content="Iyin,yinwein@foxmail.com"><meta name="copyright" content="Iyin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/plant.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="2.2 Transformer"><meta name="twitter:description" content="基于transformers的自然语言处理(NLP)入门"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="2.2 Transformer"><meta property="og:url" content="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.2/"><meta property="og:site_name" content="Iyin's blog"><meta property="og:description" content="基于transformers的自然语言处理(NLP)入门"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.2/"><link rel="next" title="2.1 Attention" href="http://yoursite.com/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.1/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web:600&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Iyin's blog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://ae01.alicdn.com/kf/Hb280d3c31450463cb3dc638bf33ca871f.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">13</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#前言"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">前言</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#从整体宏观来理解-Transformer"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">从整体宏观来理解 Transformer</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#从细节来理解-Transformer"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">从细节来理解 Transformer</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Transformer-的输入"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">Transformer 的输入</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Encoder-编码器"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text">Encoder(编码器)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Self-Attention-整体理解"><span class="toc_mobile_items-number">2.1.3.</span> <span class="toc_mobile_items-text">Self-Attention 整体理解</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Self-Attention-的细节"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">Self-Attention 的细节</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#计算Query-向量，Key-向量，Value-向量"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">计算Query 向量，Key 向量，Value 向量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#计算-Attention-Score（注意力分数）"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">计算 Attention Score（注意力分数）</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#使用矩阵计算-Self-Attention"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">使用矩阵计算 Self-Attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#多头注意力机制（multi-head-attention）"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">多头注意力机制（multi-head attention）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码实现矩阵计算-Attention"><span class="toc_mobile_items-number">2.5.</span> <span class="toc_mobile_items-text">代码实现矩阵计算 Attention</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#码"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">码</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#从整体宏观来理解-Transformer"><span class="toc-number">2.</span> <span class="toc-text">从整体宏观来理解 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#从细节来理解-Transformer"><span class="toc-number">2.1.</span> <span class="toc-text">从细节来理解 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-的输入"><span class="toc-number">2.1.1.</span> <span class="toc-text">Transformer 的输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-编码器"><span class="toc-number">2.1.2.</span> <span class="toc-text">Encoder(编码器)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Attention-整体理解"><span class="toc-number">2.1.3.</span> <span class="toc-text">Self-Attention 整体理解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention-的细节"><span class="toc-number">2.2.</span> <span class="toc-text">Self-Attention 的细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#计算Query-向量，Key-向量，Value-向量"><span class="toc-number">2.2.1.</span> <span class="toc-text">计算Query 向量，Key 向量，Value 向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#计算-Attention-Score（注意力分数）"><span class="toc-number">2.2.2.</span> <span class="toc-text">计算 Attention Score（注意力分数）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用矩阵计算-Self-Attention"><span class="toc-number">2.3.</span> <span class="toc-text">使用矩阵计算 Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多头注意力机制（multi-head-attention）"><span class="toc-number">2.4.</span> <span class="toc-text">多头注意力机制（multi-head attention）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现矩阵计算-Attention"><span class="toc-number">2.5.</span> <span class="toc-text">代码实现矩阵计算 Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#码"><span class="toc-number">3.</span> <span class="toc-text">码</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://s2.ax1x.com/2020/02/27/3aTBOx.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">2.2 Transformer</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2021-08-18<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2021-08-19</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><p>搬运自<a href="https://github.com/datawhalechina/learn-nlp-with-transformers.git" target="_blank" rel="noopener">https://github.com/datawhalechina/learn-nlp-with-transformers.git</a></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>Transformer 依赖于 Self Attention 的知识。</strong>Attention 是一种在深度学习中广泛使用的方法，Attention的思想提升了机器翻译的效果。</p>
<p>2017 年，Google 提出了 Transformer 模型，用 Self Attention 的结构，取代了以往 NLP 任务中的 RNN 网络结构，在 WMT 2014 Englishto-German 和 WMT 2014 English-to-French两个机器翻译任务上都取得了当时 SOTA 的效果。</p>
<p>这个模型的其中一个优点，就是<strong>使得模型训练过程能够并行计算</strong>。在 RNN 中，每一个 time step 的计算都依赖于上一个 time step 的输出，这就使得<strong>所有的 time step 必须串行化，无法并行计算</strong>，如下图所示。</p>
<p><img alt="机器翻译示意图" data-src="/images/2-translation.png" src="/img/loading.gif" class="lazyload"><br>图：机器翻译示意图</p>
<p>而<strong>在 Transformer 中，所有 time step 的数据，都是经过 Self Attention 计算，使得整个运算过程可以并行化计算。</strong></p>
<p><strong>Transformer 使用了 Seq2Seq任务中常用的结构——包括两个部分：Encoder 和 Decoder。</strong>一般的结构图，都是像下面这样。</p>
<p><img alt="transformer" data-src="/images/2-transformer.png" src="/img/loading.gif" class="lazyload"><br>图：transformer</p>
<h1 id="从整体宏观来理解-Transformer"><a href="#从整体宏观来理解-Transformer" class="headerlink" title="从整体宏观来理解 Transformer"></a>从整体宏观来理解 Transformer</h1><p>首先，我们将整个模型视为黑盒。在机器翻译任务中，接收一种语言的句子作为输入，然后将其翻译成其他语言输出。</p>
<p><img alt="input-output" data-src="/images/2-input-output.png" src="/img/loading.gif" class="lazyload"><br>图：input-output</p>
<p>中间部分的 Transformer 可以拆分为 2 部分：左边是编码部分(encoding component)，右边是解码部分(decoding component)。<br><img alt="encoder-decoder" data-src="/images/2-encoder-decoder.png" src="/img/loading.gif" class="lazyload"><br>图：encoder-decoder</p>
<p>其中<strong>编码部分是多层的编码器(Encoder)组成（Transformer 的论文中使用了 6 层编码器，这里的层数 6 并不是固定的，你也可以根据实验效果来修改层数）</strong>。同理，<strong>解码部分也是由多层的解码器(Decoder)组成（论文里也使用了 6 层的解码器）</strong>。<br><img alt="翻译例子" data-src="/images/2-encoder-decoder.png" src="/img/loading.gif" class="lazyload"></p>
<p>图：翻译例子</p>
<p>encoder由多层编码器组成，<strong>每层编码器在结构上都是一样的，但不同层编码器的权重参数是不同的</strong>。每层<strong>编码器</strong>里面，主要由以下两部分组成</p>
<ul>
<li>Self-Attention Layer</li>
<li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）</li>
</ul>
<p><img alt="encoder" data-src="/images/2-encoder.png" src="/img/loading.gif" class="lazyload"></p>
<p>图：单层transformer encoder</p>
<p>输入编码器的<strong>文本数据</strong>，<strong>首先会经过一个 Self Attention 层，这个层处理一个词的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息</strong>（你可以类比为：当我们翻译一个词的时候，不仅会只关注当前的词，也会关注这个词的上下文的其他词的信息）。本文后面将会详细介绍 Self Attention 的内部结构。</p>
<p>接下来，Self Attention 层的输出会经过<strong>前馈神经网络</strong>。</p>
<p>同理，<strong>解码器</strong>也具有这两层，但是<strong>这两层中间还插入了一个 Encoder-Decoder Attention 层，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）</strong>。</p>
<p><img alt="decoder" data-src="/images/2-decoder.webp" src="/img/loading.gif" class="lazyload"></p>
<p>图：decoder<br>以上便是在机器翻译任务里，宏观上的的transformer。</p>
<h2 id="从细节来理解-Transformer"><a href="#从细节来理解-Transformer" class="headerlink" title="从细节来理解 Transformer"></a>从细节来理解 Transformer</h2><p>上面，我们从宏观理解了 Transformer 的主要部分。下面，我们来看输入的张量数据，在 Transformer 中运算最终得到输出的过程。</p>
<h3 id="Transformer-的输入"><a href="#Transformer-的输入" class="headerlink" title="Transformer 的输入"></a>Transformer 的输入</h3><p>和通常的 NLP 任务一样，我们首先会使用词嵌入算法（embedding algorithm），<strong>将每个词转换为一个词向量。实际中向量一般是 256 或者 512 维</strong>。为了简化起见，这里将每个词的转换为一个 4 维的词向量。</p>
<p>那么整个输入的句子是一个向量列表，其中有 3 个词向量。在实际中，<strong>每个句子的长度不一样，我们会取一个适当的值，作为向量列表的长度。如果一个句子达不到这个长度，那么就填充全为 0 的词向量；如果句子超出这个长度，则做截断。</strong>句子长度是一个超参数，通常是训练集中的句子的最大长度，你可以尝试不同长度的效果。</p>
<p><img alt=" 个词向量" data-src="/images/2-x.png" src="/img/loading.gif" class="lazyload"><br>图：3个词向量</p>
<h3 id="Encoder-编码器"><a href="#Encoder-编码器" class="headerlink" title="Encoder(编码器)"></a>Encoder(编码器)</h3><p>编码器（Encoder）接收的输入都是一个向量列表，输出也是<strong>大小同样的向量列表</strong>，然后接着输入下一个编码器。</p>
<p>第一 个/层 编码器的输入是词向量，<em>而后面的编码器的输入是上一个编码器的输出</em>。</p>
<p>下面，我们来看这个向量列表在编码器里面是如何流动的。</p>
<p><img alt="输入encoder" data-src="/images/2-x-encoder.png" src="/img/loading.gif" class="lazyload"><br>图：输入encoder</p>
<p>每个单词转换成一个向量之后，进入self-attention层，<strong>每个位置的单词得到新向量</strong>，然后再输入FFN神经网络。</p>
<p>下面再看一个2个单词的例子：<br><img alt="一层传一层" data-src="/images/2-multi-encoder.webp" src="/img/loading.gif" class="lazyload"><br>图：一层传一层</p>
<p>每个位置的词都经过 Self Attention 层，得到的<strong>每个输出向量都单独经过前馈神经网络层，每个向量经过的前馈神经网络都是一样的</strong>。</p>
<h3 id="Self-Attention-整体理解"><a href="#Self-Attention-整体理解" class="headerlink" title="Self-Attention 整体理解"></a>Self-Attention 整体理解</h3><p>假设我们想要翻译的句子是：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">The animal didn't cross the street because it was too tired</span><br></pre></td></tr></tbody></table></figure></div>

<p>这个句子中的 <em>it</em> 是一个指代词，那么 <em>it</em> 指的是什么呢？它是指 <em>animal</em> 还是<em>street</em>？这个问题对人来说，是很简单的，但是对算法来说并不是那么容易。</p>
<p>当模型在处理（翻译）it 的时候，<em>Self Attention</em>机制能够让模型把it和animal关联起来。</p>
<p>同理，<strong>当模型处理句子中的每个词时，<em>Self Attentio</em>n机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词</strong>，作为辅助线索，进而可以更好地<strong>编码</strong>当前位置的词。</p>
<p>如果你熟悉 RNN，回忆一下：RNN 在处理一个词时，会考虑前面传过来的<em>hidden state</em>，而<em>hidden state</em>就包含了前面的词的信息。而 <strong>Transformer 使用<em>Self Attention</em>机制，会把其他单词的理解融入处理当前的单词</strong>。</p>
<p><img alt="一个词和其他词的attention" data-src="/images/2-attention-word.png" src="/img/loading.gif" class="lazyload"><br>图：一个词和其他词的attention</p>
<p>如上图可视化图所示，当我们在第五层编码器中（编码部分中的最后一层编码器）编码“it”时，有一部分注意力集中在“The animal”上，并且把这两个词的信息融合到了”it”这个单词中。</p>
<h2 id="Self-Attention-的细节"><a href="#Self-Attention-的细节" class="headerlink" title="Self-Attention 的细节"></a>Self-Attention 的细节</h2><h3 id="计算Query-向量，Key-向量，Value-向量"><a href="#计算Query-向量，Key-向量，Value-向量" class="headerlink" title="计算Query 向量，Key 向量，Value 向量"></a>计算Query 向量，Key 向量，Value 向量</h3><p>下面我们先看下如何使用向量来计算 Self Attention，然后再看下如何使用矩阵来实现 Self Attention。（<strong>矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式</strong>）。</p>
<p>计算 Self Attention 的第 1 步是<strong>：对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量。</strong>这 3 个向量是词向量分别和 3 个矩阵相乘得到的，而这个矩阵是我们要学习的参数。</p>
<blockquote>
<p>每个词：1<em>256  *  256</em>N (一般小于原来的长度，如256）</p>
</blockquote>
<p>注意，<strong>这 3 个新得到的向量一般比原来的词向量的长度更小。</strong>假设这 3 个向量的长度是$d_{key}$，而原始的词向量或者最终输出的向量的长度是 512<strong>（**</strong>这 3 个向量的长度，和最终输出的向量长度，是有倍数关系的）**。关于 Multi-head Attention，后面会给出实际代码。这里为了简化，假设只有一个 head 的 Self-Attention。</p>
<p><img alt="Q,K,V" data-src="/images/2-qkv.png" src="/img/loading.gif" class="lazyload">图：Q,K,V</p>
<p>上图中，有两个词向量：Thinking 的词向量 x1 和 Machines 的词向量 x2。以 x1 为例，X1 乘以 WQ 得到 q1，q1 就是 X1 对应的 Query 向量。同理，X1 乘以 WK 得到 k1，k1 是 X1 对应的 Key 向量；X1 乘以 WV 得到 v1，<strong>v1 是 X1 对应的 Value 向量</strong>。</p>
<p>Query 向量，Key 向量，Value 向量是什么含义呢？</p>
<p>其实它们就是 3 个向量，给它们加上一个名称，可以让我们更好地理解 Self-Attention 的计算过程和逻辑含义。继续往下读，你会知道 attention 是如何计算出来的，Query 向量，Key 向量，Value 向量又分别扮演了什么角色。</p>
<h3 id="计算-Attention-Score（注意力分数）"><a href="#计算-Attention-Score（注意力分数）" class="headerlink" title="计算 Attention Score（注意力分数）"></a>计算 Attention Score（注意力分数）</h3><p>第 2 步，是<strong>计算 Attention Score（注意力分数）</strong>。假设我们现在计算第一个词 <em>Thinking</em> 的 Attention Score（注意力分数），需要根据 <em>Thinking</em> 这个词，<strong>对句子中的其他每个词都计算一个分数</strong>。这些分数决定了我们<strong>在编码<em>Thinking</em>这个词时，需要对句子中其他位置的每个词放置多少的注意力</strong>。</p>
<p>这些分数，是通过计算 “<em>Thinking</em>“ 对应的 <strong>Query 向量和其他位置的每个词的 Key 向量的点积</strong>，而得到的。如果我们计算句子中第一个位置单词的 Attention Score（注意力分数），那么第一个分数就是 q1 和 k1 的内积，第二个分数就是 q1 和 k2 的点积。</p>
<p><img alt="Thinking计算" data-src="/images/2-think.png" src="/img/loading.gif" class="lazyload"><br>图：Thinking计算</p>
<p>第 3 步就是把每个分数除以 $\sqrt(d_{key})$ （$d_{key}$是 Key 向量的长度）。你也可以除以其他数，除以一个数是为了在反向传播时，求取梯度更加稳定。</p>
<p>第 4 步，接着把这些分数经过一个 Softmax 层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于 1。</p>
<p><img alt="Thinking计算" data-src="/images/2-think2.png" src="/img/loading.gif" class="lazyload"><br>图：Thinking计算</p>
<p>这些分数决定了在编码当前位置（这里的例子是第一个位置）的词时，对所有位置的词分别有多少的注意力。很明显，在上图的例子中，当前位置（这里的例子是第一个位置）的词会有最高的分数，但有时，关注到其他位置上相关的词也很有用。</p>
<p>第 5 步，得到每个位置的分数后，<strong>将每个分数分别与每个 Value 向量相乘</strong>。这种做法背后的直觉理解就是：<strong>对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，</strong>这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。</p>
<p>第 6 步是<strong>把上一步得到的向量相加，就得到了 Self Attention 层在这个位置（这里的例子是第一个位置）的输出。</strong></p>
<p><img alt="Think计算" data-src="/images/2-sum.png" src="/img/loading.gif" class="lazyload"><br>图：Think计算</p>
<p>上面这张图，包含了 Self Attention 的全过程，<strong>最终得到的当前位置（这里的例子是第一个位置）的向量会输入到前馈神经网络</strong>。但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。下面让我们来看，如何使用矩阵来计算所有位置的输出向量。</p>
<h2 id="使用矩阵计算-Self-Attention"><a href="#使用矩阵计算-Self-Attention" class="headerlink" title="使用矩阵计算 Self-Attention"></a>使用矩阵计算 Self-Attention</h2><p>第一步是计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵 X 中，然后分别和3 个权重矩阵$W^Q, W^K W^V$ 相乘，得到 Q，K，V 矩阵。</p>
<p><img alt data-src="/images/2-qkv-multi.png" src="/img/loading.gif" class="lazyload">图：QKV矩阵乘法</p>
<p>矩阵 X 中的每一行，表示句子中的每一个词的词向量，长度是 512。Q，K，V 矩阵中的每一行表示 Query 向量，Key 向量，Value 向量，向量长度是 64。</p>
<p>接着，由于我们使用了矩阵来计算，我们可以<strong>把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</strong></p>
<p><img alt="输出" data-src="/images/2-attention-output.webp" src="/img/loading.gif" class="lazyload"><br>图：输出</p>
<h2 id="多头注意力机制（multi-head-attention）"><a href="#多头注意力机制（multi-head-attention）" class="headerlink" title="多头注意力机制（multi-head attention）"></a>多头注意力机制（multi-head attention）</h2><p>Transformer 的论文通过增加<strong>多头注意力机制（一组注意力称为一个 attention head）</strong>，进一步完善了 Self Attention 层。这种机制从如下两个方面增强了 attention 层的能力：</p>
<ul>
<li>它<strong>扩展了模型关注不同位置的能力</strong>。在上面的例子中，第一个位置的输出 z1 包含了句子中其他每个位置的很小一部分信息，但 z1 可能主要是由第一个位置的信息决定的。当我们翻译句子：<code>The animal didn’t cross the street because it was too tired</code>时，我们想让机器知道其中的it指代的是什么。这时，多头注意力机制会有帮助。</li>
<li><strong>多头注意力机制赋予 attention 层多个“子表示空间”。</strong>下面我们会看到，多头注意力机制会有多组$W^Q, W^K W^V$ 的权重矩阵（<strong>在 Transformer 的论文中，使用了 8 组注意力（attention heads）</strong>。因此，接下来我也是用 8 组注意力头 （attention heads））。每一组注意力的 的权重矩阵都是随机初始化的。<strong>经过训练之后，每一组注意力$W^Q, W^K W^V$ 可以看作是把输入的向量映射到一个”子表示空间“。</strong></li>
</ul>
<p><img alt="多头注意力机制" data-src="/images/2-multi-head.png" src="/img/loading.gif" class="lazyload"><br>图：多头注意力机制</p>
<p><strong>在多头注意力机制中，我们为每组注意力维护单独的 WQ, WK, WV 权重矩阵。将输入 X 和每组注意力的WQ, WK, WV 相乘，得到 8 组 Q, K, V 矩阵。</strong></p>
<p><strong>接着，我们把每组 K, Q, V 计算得到每组的 Z 矩阵，就得到 8 个 Z 矩阵。</strong></p>
<p><img alt="8 个 Z 矩阵" data-src="/images/2-8z.webp" src="/img/loading.gif" class="lazyload"><br>图：8 个 Z 矩阵</p>
<p>接下来就有点麻烦了，因为前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵。所以我们需要一种方法，把 8 个矩阵整合为一个矩阵。</p>
<p>怎么才能做到呢？我们<strong>把矩阵拼接起来，然后和另一个权重矩阵$W^Q$相乘。</strong></p>
<p><img alt="整合矩阵" data-src="/images/2-to1.webp" src="/img/loading.gif" class="lazyload"><br>图：整合矩阵</p>
<ol>
<li>把 8 个矩阵 {Z0,Z1…,Z7} 拼接起来</li>
<li>把拼接后的矩阵和 WO 权重矩阵相乘</li>
<li>得到<strong>最终的矩阵 Z，这个矩阵包含了所有 attention heads（注意力头） 的信息。这个矩阵会输入到 FFNN (Feed Forward Neural Network)层</strong>。</li>
</ol>
<p>这就是多头注意力的全部内容。我知道，在上面的讲解中，出现了相当多的矩阵。下面我把所有的内容都放到一张图中，这样你可以总揽全局，在这张图中看到所有的内容。</p>
<p><img alt="放在一起" data-src="/images/2-put-together.webp" src="/img/loading.gif" class="lazyload"><br>图：放在一起</p>
<p>既然我们已经谈到了多头注意力，现在让我们重新回顾之前的翻译例子，看下当我们编码单词it时，不同的 attention heads （注意力头）关注的是什么部分。</p>
<p><img alt="`it`的attention" data-src="/images/2-it-attention.webp" src="/img/loading.gif" class="lazyload"><br>图：<code>it</code>的attention</p>
<p>当我们编码单词”it”时，其中一个 attention head （注意力头）最关注的是”the animal”，另外一个 attention head 关注的是”tired”。因此在某种意义上，”it”在模型中的表示，融合了”animal”和”word”的部分表达。</p>
<blockquote>
<p>每个注意力头怎么保证注意力能够偏向合理的方向和不同的方向呢？</p>
</blockquote>
<p>然而，当我们把所有 attention heads（注意力头） 都在图上画出来时，多头注意力又变得难以解释了。</p>
<p><img alt="所有注意力heads" data-src="/images/2-all-att.png" src="/img/loading.gif" class="lazyload"><br>图：所有注意力heads</p>
<h2 id="代码实现矩阵计算-Attention"><a href="#代码实现矩阵计算-Attention" class="headerlink" title="代码实现矩阵计算 Attention"></a>代码实现矩阵计算 Attention</h2><p>下面我们是用代码来演示，如何使用矩阵计算 attention。首先使用 PyTorch 库提供的函数实现，然后自己再实现。</p>
<p>PyTorch 提供了 MultiheadAttention 来实现 attention 的计算。</p>
<h1 id="码"><a href="#码" class="headerlink" title="码"></a>码</h1><p>本文翻译自<a href="http://jalammar.github.io/illustrated-transformer" target="_blank" rel="noopener">illustrated-transformer</a>，是笔者看过Transformer 讲解得最好的文章。这篇文章从输入开始，一步一步演示了数据在 Transformer 中的流动过程。</p>
</body></html></div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/transformers/">transformers    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2021/08/18/2021-08-18-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86(NLP)%E5%85%A5%E9%97%A82.1/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>2.1 Attention</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/08/15/2021-08-15-基于transformers的自然语言处理(NLP)入门1/" title="（一）Transformers在NLP中的兴起"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-08-15</div><div class="relatedPosts_title">（一）Transformers在NLP中的兴起</div></div></a></div><div class="relatedPosts_item"><a href="/2021/08/18/2021-08-18-基于transformers的自然语言处理(NLP)入门2.1/" title="2.1 Attention"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-08-18</div><div class="relatedPosts_title">2.1 Attention</div></div></a></div><div class="relatedPosts_item"><a href="/2021/02/21/2021-02-21-NLP中文预训练模型泛化能力挑战赛（01_Docker提交）/" title="NLP中文预训练模型泛化能力挑战赛（01 Docker提交）"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-02-21</div><div class="relatedPosts_title">NLP中文预训练模型泛化能力挑战赛（01 Docker提交）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Iyin</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script id="canvas_nest" color="189,207,69" opacity="0.9" zIndex="-1" count="70" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>